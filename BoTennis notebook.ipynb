{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import joblib\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(jsonFile):\n",
    "    with open(jsonFile) as file:\n",
    "        Json_data = json.loads(file.read())\n",
    "    return Json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_doc('intents.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_data(feat_1,feat_2,is_pattern):\n",
    "    is_pattern = is_pattern\n",
    "    df = pd.DataFrame(columns=[feat_1,feat_2])\n",
    "    for intent in data['intents']:\n",
    "        if is_pattern:\n",
    "            for pattern in intent['patterns']:\n",
    "                w = pattern\n",
    "                df_to_append = pd.Series([w,intent['tag']], index = df.columns)\n",
    "                df = df.append(df_to_append,ignore_index=True)\n",
    "        else:\n",
    "            for response in intent['responses']:\n",
    "                w = response\n",
    "                df_to_append = pd.Series([w,intent['tag']], index = df.columns)\n",
    "                df = df.append(df_to_append,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi there</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is anyone there?</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hola</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>What are the match formats in tennis</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Do we just keep playing till we get tired</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>How many match sets are there in tennis</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>When do we stop playing?</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>How many set can be played?</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    questions              labels\n",
       "0                                    Hi there  start_conversation\n",
       "1                            Is anyone there?  start_conversation\n",
       "2                                         Hey  start_conversation\n",
       "3                                        Hola  start_conversation\n",
       "4                                       Hello  start_conversation\n",
       "..                                        ...                 ...\n",
       "82       What are the match formats in tennis        match_format\n",
       "83  Do we just keep playing till we get tired        match_format\n",
       "84    How many match sets are there in tennis        match_format\n",
       "85                   When do we stop playing?        match_format\n",
       "86                How many set can be played?        match_format\n",
       "\n",
       "[87 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# users intents \n",
    "df1 = frame_data('questions','labels',True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bot_scope              5\n",
       "court_lines            3\n",
       "start_conversation     7\n",
       "kits                  10\n",
       "options                8\n",
       "thanks                 5\n",
       "what_are_you           4\n",
       "serving_rules          6\n",
       "tie_break              5\n",
       "first_server           4\n",
       "tennis_games           4\n",
       "top_players            6\n",
       "match_format           5\n",
       "end_conversation       5\n",
       "general_rules          5\n",
       "scoring                5\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.labels.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, I'm sure you love tennis</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy to have you here</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good to see you again</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi, I'm  BoTennis</td>\n",
       "      <td>what_are_you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          response              labels\n",
       "0  Hello, I'm sure you love tennis  start_conversation\n",
       "1           Happy to have you here  start_conversation\n",
       "2            Good to see you again  start_conversation\n",
       "3        Hi there, how can I help?  start_conversation\n",
       "4                Hi, I'm  BoTennis        what_are_you"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bot response\n",
    "df2 = frame_data('response','labels',False)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "vocab = Counter()\n",
    "labels = []\n",
    "def tokenizer(entry):\n",
    "    tokens = entry.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens]\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokenizer,df,feature):\n",
    "    doc_without_stopwords = []\n",
    "    for entry in df[feature]:\n",
    "        tokens = tokenizer(entry)\n",
    "        joblib.dump(tokens,'tokens.pkl')\n",
    "        doc_without_stopwords.append(' '.join(tokens))\n",
    "    df[feature] = doc_without_stopwords\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(tokenizer,df,feature):\n",
    "    for entry in df[feature]:\n",
    "        tokens = tokenizer(entry)   \n",
    "        vocab.update(tokens)\n",
    "    joblib.dump(vocab,'vocab.pkl')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vocab(tokenizer,df1,'questions')\n",
    "remove_stop_words(tokenizer,df1,'questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tennis', 36), ('the', 24), ('what', 23), ('you', 22), ('are', 19), ('do', 16), ('in', 11), ('rule', 11), ('how', 10), ('can', 7), ('know', 7), ('about', 7), ('is', 6), ('player', 6), ('list', 6), ('service', 6), ('score', 6), ('who', 5), ('me', 5), ('kit', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi there</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is anyone there</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hola</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>what are the match format in tennis</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>do we just keep playing till we get tired</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>how many match set are there in tennis</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>when do we stop playing</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>how many set can be played</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    questions              labels\n",
       "0                                    hi there  start_conversation\n",
       "1                             is anyone there  start_conversation\n",
       "2                                         hey  start_conversation\n",
       "3                                        hola  start_conversation\n",
       "4                                       hello  start_conversation\n",
       "..                                        ...                 ...\n",
       "82        what are the match format in tennis        match_format\n",
       "83  do we just keep playing till we get tired        match_format\n",
       "84     how many match set are there in tennis        match_format\n",
       "85                    when do we stop playing        match_format\n",
       "86                 how many set can be played        match_format\n",
       "\n",
       "[87 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what do you know about tennis',\n",
       " 'each line on the tennis court mean whats',\n",
       " 'bye',\n",
       " 'who serf first',\n",
       " 'what are the general rule of tennis',\n",
       " 'what are the tennis kit need to get started',\n",
       " 'what are the match format in tennis',\n",
       " 'how can you help me',\n",
       " 'how is score counted',\n",
       " 'what are the service rule',\n",
       " 'hi there',\n",
       " 'do you know tennis game type',\n",
       " 'thanks',\n",
       " 'what will happen if game end in same score',\n",
       " 'who are the top player in tennis',\n",
       " 'what is your name']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = list(df1.groupby(by='labels',as_index=False).first()['questions'])\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 55, 11, 58, 40, 45, 82, 21, 77, 62, 0, 68, 16, 72, 34, 7]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = []\n",
    "for i,_ in enumerate(test_list):\n",
    "    idx = df1[df1.questions == test_list[i]].index[0]\n",
    "    test_index.append(idx)\n",
    "test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = [i for i in df1.index if i not in test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi there is anyone hey hola hello good day what your name are you who pls bye see later goodbye nice chatting to till next time thanks thank thats helpful awesome for helping me how can help do provide be support offered know about tennis tell really lot exactly mean by guide through the top player in list best general rule of playing kit need get started wear beginner full will each line on court whats why so many where base serf first decide serve anybody service stand anywhere game type different explain happen if end same score tiebreak doe work winner when tie break counted scoring board count match format we just keep tired set stop played'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df,feature):\n",
    "#     text = ' '.join(list(vocab.keys()))\n",
    "    t = Tokenizer()\n",
    "    entries = [entry for entry in df[feature]]\n",
    "    t.fit_on_texts(entries)\n",
    "    joblib.dump(t,'tokenizer_t.pkl')\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    entries = [entry for entry in df[feature]]\n",
    "    max_length = max([len(s.split()) for s in entries])\n",
    "    encoded = t.texts_to_sequences(entries)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,vocab_size = encoder(df1,'questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>65</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2  3  4  5  6  7  8  9  10  11  12  13  14  15              labels\n",
       "0  44  23   0  0  0  0  0  0  0  0   0   0   0   0   0   0  start_conversation\n",
       "1  13  65  23  0  0  0  0  0  0  0   0   0   0   0   0   0  start_conversation\n",
       "2  66   0   0  0  0  0  0  0  0  0   0   0   0   0   0   0  start_conversation\n",
       "3  67   0   0  0  0  0  0  0  0  0   0   0   0   0   0   0  start_conversation\n",
       "4  68   0   0  0  0  0  0  0  0  0   0   0   0   0   0   0  start_conversation"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded['labels'] = df1.labels\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    dt = [0]*16\n",
    "    dt.append('confused')\n",
    "    dt = [dt]\n",
    "    pd.DataFrame(dt).rename(columns = {16:'labels'})\n",
    "    df_encoded = df_encoded.append(pd.DataFrame(dt).rename(columns = {16:'labels'}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>61</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>117</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>match_format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>confused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>confused</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2    3   4    5  6  7  8  9  10  11  12  13  14  15        labels\n",
       "84   9  40  63   64   5   23  7  1  0  0   0   0   0   0   0   0  match_format\n",
       "85  61   6  43  117  36    0  0  0  0  0   0   0   0   0   0   0  match_format\n",
       "86   9  40  64   10  25  118  0  0  0  0   0   0   0   0   0   0  match_format\n",
       "87   0   0   0    0   0    0  0  0  0  0   0   0   0   0   0   0      confused\n",
       "88   0   0   0    0   0    0  0  0  0  0   0   0   0   0   0   0      confused"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index.append(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index.append(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded = df_encoded.append(pd.DataFrame(dt).rename(columns = {16:'labels'}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lable_enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 11, 11, 11, 11, 11, 11, 16, 16, 16, 16,  3,  3,  3,  3,  3, 13,\n",
       "       13, 13, 13, 13,  8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,\n",
       "       15, 15, 15, 15, 15, 15,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  2,  2,  2,  4,  4,  4,  4, 10, 10, 10, 10, 10, 10,\n",
       "       12, 12, 12, 12, 14, 14, 14, 14, 14,  9,  9,  9,  9,  9,  7,  7,  7,\n",
       "        7,  7,  1,  1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labl = lable_enc.fit_transform(df_encoded.labels)\n",
    "labl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_conversation': 11,\n",
       " 'what_are_you': 16,\n",
       " 'end_conversation': 3,\n",
       " 'thanks': 13,\n",
       " 'options': 8,\n",
       " 'bot_scope': 0,\n",
       " 'top_players': 15,\n",
       " 'general_rules': 5,\n",
       " 'kits': 6,\n",
       " 'court_lines': 2,\n",
       " 'first_server': 4,\n",
       " 'serving_rules': 10,\n",
       " 'tennis_games': 12,\n",
       " 'tie_break': 14,\n",
       " 'scoring': 9,\n",
       " 'match_format': 7,\n",
       " 'confused': 1}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper = {}\n",
    "for index,key in enumerate(df_encoded.labels):\n",
    "    if key not in mapper.keys():\n",
    "        mapper[key] = labl[index]\n",
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, I'm sure you love tennis</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy to have you here</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good to see you again</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>start_conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi, I'm  BoTennis</td>\n",
       "      <td>what_are_you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          response              labels\n",
       "0  Hello, I'm sure you love tennis  start_conversation\n",
       "1           Happy to have you here  start_conversation\n",
       "2            Good to see you again  start_conversation\n",
       "3        Hi there, how can I help?  start_conversation\n",
       "4                Hi, I'm  BoTennis        what_are_you"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, I'm sure you love tennis</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy to have you here</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good to see you again</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi, I'm  BoTennis</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          response  labels\n",
       "0  Hello, I'm sure you love tennis      11\n",
       "1           Happy to have you here      11\n",
       "2            Good to see you again      11\n",
       "3        Hi there, how can I help?      11\n",
       "4                Hi, I'm  BoTennis      16"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.labels = df2.labels.map(mapper).astype({'labels': 'int32'})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('response.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.groupby('labels').get_group(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_encoded.loc[train_index]\n",
    "test = df_encoded.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['labels'],axis=1)\n",
    "y_train = train.labels\n",
    "X_test = test.drop(columns=['labels'],axis=1)\n",
    "y_test = test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =pd.get_dummies(y_train).values\n",
    "y_test =pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17,), (17,))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape,y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 16)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = X_train.shape[1]\n",
    "# output = len(df3.labels.unique())\n",
    "output = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_model(vocab_size, max_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size,300, input_length=max_length))\n",
    "#     model.add(Conv1D(filters=64, kernel_size=6, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=4))\n",
    "#     model.add(Flatten())\n",
    "# #     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(16, activation='softmax'))\n",
    "    \n",
    "#     # compile network\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     # summarize defined model\n",
    "#     model.summary()\n",
    "# #     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "checkpoint = ModelCheckpoint(\"model-v1.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "callbacks = [early_stopping,checkpoint,reduce_lr]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,300, input_length=max_length))\n",
    "    model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=8))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(17, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # compile network\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "              # optimizer = Adam(lr=0.001),\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YHEMMY\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From C:\\Users\\YHEMMY\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 300)           35700     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 13, 64)            76864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 17)                1105      \n",
      "=================================================================\n",
      "Total params: 113,669\n",
      "Trainable params: 113,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72 samples, validate on 17 samples\n",
      "WARNING:tensorflow:From C:\\Users\\YHEMMY\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From C:\\Users\\YHEMMY\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "64/72 [=========================>....] - ETA: 0s - loss: 2.8364 - acc: 0.0625\n",
      "Epoch 00001: val_loss improved from inf to 2.80114, saving model to model-v1.h5\n",
      "72/72 [==============================] - 4s 54ms/sample - loss: 2.8359 - acc: 0.0833 - val_loss: 2.8011 - val_acc: 0.1176\n",
      "Epoch 2/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.7485 - acc: 0.3125\n",
      "Epoch 00002: val_loss improved from 2.80114 to 2.77412, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 7ms/sample - loss: 2.7487 - acc: 0.2639 - val_loss: 2.7741 - val_acc: 0.1176\n",
      "Epoch 3/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.7004 - acc: 0.2812\n",
      "Epoch 00003: val_loss improved from 2.77412 to 2.74670, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 2.6843 - acc: 0.3194 - val_loss: 2.7467 - val_acc: 0.1176\n",
      "Epoch 4/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.6645 - acc: 0.3125\n",
      "Epoch 00004: val_loss improved from 2.74670 to 2.71540, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 2.6251 - acc: 0.4167 - val_loss: 2.7154 - val_acc: 0.1176\n",
      "Epoch 5/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.5688 - acc: 0.5625\n",
      "Epoch 00005: val_loss improved from 2.71540 to 2.68433, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 2.5636 - acc: 0.5000 - val_loss: 2.6843 - val_acc: 0.1176\n",
      "Epoch 6/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.5194 - acc: 0.5000\n",
      "Epoch 00006: val_loss improved from 2.68433 to 2.64971, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 2.4970 - acc: 0.5278 - val_loss: 2.6497 - val_acc: 0.2353\n",
      "Epoch 7/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.4753 - acc: 0.5312\n",
      "Epoch 00007: val_loss improved from 2.64971 to 2.61361, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 2.4263 - acc: 0.5694 - val_loss: 2.6136 - val_acc: 0.2941\n",
      "Epoch 8/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.3439 - acc: 0.6250\n",
      "Epoch 00008: val_loss improved from 2.61361 to 2.57312, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 2.3509 - acc: 0.6111 - val_loss: 2.5731 - val_acc: 0.2941\n",
      "Epoch 9/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.2990 - acc: 0.6562\n",
      "Epoch 00009: val_loss improved from 2.57312 to 2.53001, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 2.2699 - acc: 0.6250 - val_loss: 2.5300 - val_acc: 0.2941\n",
      "Epoch 10/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.1996 - acc: 0.7188\n",
      "Epoch 00010: val_loss improved from 2.53001 to 2.48124, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 2.1857 - acc: 0.6667 - val_loss: 2.4812 - val_acc: 0.3529\n",
      "Epoch 11/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 2.0927 - acc: 0.7188\n",
      "Epoch 00011: val_loss improved from 2.48124 to 2.42997, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 2ms/sample - loss: 2.0945 - acc: 0.6806 - val_loss: 2.4300 - val_acc: 0.3529\n",
      "Epoch 12/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.9696 - acc: 0.6562\n",
      "Epoch 00012: val_loss improved from 2.42997 to 2.37373, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 2.0002 - acc: 0.7083 - val_loss: 2.3737 - val_acc: 0.4118\n",
      "Epoch 13/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.8892 - acc: 0.8438\n",
      "Epoch 00013: val_loss improved from 2.37373 to 2.31551, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 1.9038 - acc: 0.8056 - val_loss: 2.3155 - val_acc: 0.5294\n",
      "Epoch 14/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.8791 - acc: 0.7500\n",
      "Epoch 00014: val_loss improved from 2.31551 to 2.25497, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 1.8009 - acc: 0.8194 - val_loss: 2.2550 - val_acc: 0.5294\n",
      "Epoch 15/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.7766 - acc: 0.9062\n",
      "Epoch 00015: val_loss improved from 2.25497 to 2.18849, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 1.6936 - acc: 0.8750 - val_loss: 2.1885 - val_acc: 0.5294\n",
      "Epoch 16/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.5392 - acc: 0.9062\n",
      "Epoch 00016: val_loss improved from 2.18849 to 2.11668, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 1.5858 - acc: 0.8750 - val_loss: 2.1167 - val_acc: 0.5294\n",
      "Epoch 17/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.5155 - acc: 0.9062\n",
      "Epoch 00017: val_loss improved from 2.11668 to 2.04109, saving model to model-v1.h5\n",
      "72/72 [==============================] - 1s 16ms/sample - loss: 1.4725 - acc: 0.8889 - val_loss: 2.0411 - val_acc: 0.5294\n",
      "Epoch 18/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.2401 - acc: 1.0000\n",
      "Epoch 00018: val_loss improved from 2.04109 to 1.96243, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 1.3653 - acc: 0.9028 - val_loss: 1.9624 - val_acc: 0.5294\n",
      "Epoch 19/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.1646 - acc: 0.9375\n",
      "Epoch 00019: val_loss improved from 1.96243 to 1.88640, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 1.2538 - acc: 0.9028 - val_loss: 1.8864 - val_acc: 0.5294\n",
      "Epoch 20/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.2425 - acc: 0.8438\n",
      "Epoch 00020: val_loss improved from 1.88640 to 1.81120, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 1.1473 - acc: 0.9028 - val_loss: 1.8112 - val_acc: 0.5294\n",
      "Epoch 21/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.1185 - acc: 0.8750\n",
      "Epoch 00021: val_loss improved from 1.81120 to 1.73846, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 1.0474 - acc: 0.9167 - val_loss: 1.7385 - val_acc: 0.5882\n",
      "Epoch 22/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.9171 - acc: 0.9375\n",
      "Epoch 00022: val_loss improved from 1.73846 to 1.66883, saving model to model-v1.h5\n",
      "72/72 [==============================] - 1s 7ms/sample - loss: 0.9494 - acc: 0.9167 - val_loss: 1.6688 - val_acc: 0.5882\n",
      "Epoch 23/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 1.0438 - acc: 0.9375\n",
      "Epoch 00023: val_loss improved from 1.66883 to 1.60120, saving model to model-v1.h5\n",
      "72/72 [==============================] - 1s 9ms/sample - loss: 0.8585 - acc: 0.9722 - val_loss: 1.6012 - val_acc: 0.5882\n",
      "Epoch 24/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.7150 - acc: 1.0000\n",
      "Epoch 00024: val_loss improved from 1.60120 to 1.53753, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.7740 - acc: 0.9722 - val_loss: 1.5375 - val_acc: 0.6471\n",
      "Epoch 25/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.7064 - acc: 0.9375\n",
      "Epoch 00025: val_loss improved from 1.53753 to 1.47525, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.6937 - acc: 0.9722 - val_loss: 1.4753 - val_acc: 0.5882\n",
      "Epoch 26/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.6444 - acc: 0.9688\n",
      "Epoch 00026: val_loss improved from 1.47525 to 1.41893, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.6208 - acc: 0.9722 - val_loss: 1.4189 - val_acc: 0.6471\n",
      "Epoch 27/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.7168 - acc: 0.9375\n",
      "Epoch 00027: val_loss improved from 1.41893 to 1.36795, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.5549 - acc: 0.9722 - val_loss: 1.3680 - val_acc: 0.6471\n",
      "Epoch 28/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.5100 - acc: 0.9688\n",
      "Epoch 00028: val_loss improved from 1.36795 to 1.32126, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.4912 - acc: 0.9722 - val_loss: 1.3213 - val_acc: 0.6471\n",
      "Epoch 29/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.4702 - acc: 0.9688\n",
      "Epoch 00029: val_loss improved from 1.32126 to 1.27872, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.4342 - acc: 0.9722 - val_loss: 1.2787 - val_acc: 0.7059\n",
      "Epoch 30/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.4459 - acc: 0.9375\n",
      "Epoch 00030: val_loss improved from 1.27872 to 1.24265, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.3832 - acc: 0.9722 - val_loss: 1.2426 - val_acc: 0.7059\n",
      "Epoch 31/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.4253 - acc: 0.9688\n",
      "Epoch 00031: val_loss improved from 1.24265 to 1.20570, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.3383 - acc: 0.9861 - val_loss: 1.2057 - val_acc: 0.7059\n",
      "Epoch 32/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.2514 - acc: 1.0000\n",
      "Epoch 00032: val_loss improved from 1.20570 to 1.17508, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.2999 - acc: 0.9861 - val_loss: 1.1751 - val_acc: 0.7059\n",
      "Epoch 33/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.3506 - acc: 0.9688\n",
      "Epoch 00033: val_loss improved from 1.17508 to 1.14444, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.2664 - acc: 0.9861 - val_loss: 1.1444 - val_acc: 0.6471\n",
      "Epoch 34/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.2412 - acc: 1.0000\n",
      "Epoch 00034: val_loss improved from 1.14444 to 1.11772, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.2366 - acc: 0.9861 - val_loss: 1.1177 - val_acc: 0.7059\n",
      "Epoch 35/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.2777 - acc: 0.9688\n",
      "Epoch 00035: val_loss improved from 1.11772 to 1.09573, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.2113 - acc: 0.9861 - val_loss: 1.0957 - val_acc: 0.7059\n",
      "Epoch 36/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.1536 - acc: 1.0000\n",
      "Epoch 00036: val_loss improved from 1.09573 to 1.07509, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.1905 - acc: 0.9861 - val_loss: 1.0751 - val_acc: 0.7059\n",
      "Epoch 37/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.1233 - acc: 1.0000\n",
      "Epoch 00037: val_loss improved from 1.07509 to 1.05638, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.1714 - acc: 0.9861 - val_loss: 1.0564 - val_acc: 0.7059\n",
      "Epoch 38/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.2308 - acc: 0.9688\n",
      "Epoch 00038: val_loss improved from 1.05638 to 1.03513, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 2ms/sample - loss: 0.1563 - acc: 0.9861 - val_loss: 1.0351 - val_acc: 0.7059\n",
      "Epoch 39/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0986 - acc: 1.0000\n",
      "Epoch 00039: val_loss improved from 1.03513 to 1.01656, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.1418 - acc: 0.9861 - val_loss: 1.0166 - val_acc: 0.6471\n",
      "Epoch 40/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0943 - acc: 1.0000\n",
      "Epoch 00040: val_loss improved from 1.01656 to 1.00012, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.1300 - acc: 0.9861 - val_loss: 1.0001 - val_acc: 0.7059\n",
      "Epoch 41/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0863 - acc: 1.0000\n",
      "Epoch 00041: val_loss improved from 1.00012 to 0.98391, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 6ms/sample - loss: 0.1197 - acc: 0.9861 - val_loss: 0.9839 - val_acc: 0.7059\n",
      "Epoch 42/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.1749 - acc: 0.9688\n",
      "Epoch 00042: val_loss improved from 0.98391 to 0.97011, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.1114 - acc: 0.9861 - val_loss: 0.9701 - val_acc: 0.7059\n",
      "Epoch 43/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0647 - acc: 1.0000\n",
      "Epoch 00043: val_loss improved from 0.97011 to 0.95700, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.1035 - acc: 0.9861 - val_loss: 0.9570 - val_acc: 0.7647\n",
      "Epoch 44/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.1411 - acc: 0.9688\n",
      "Epoch 00044: val_loss improved from 0.95700 to 0.94447, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0972 - acc: 0.9861 - val_loss: 0.9445 - val_acc: 0.7059\n",
      "Epoch 45/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0675 - acc: 1.0000\n",
      "Epoch 00045: val_loss improved from 0.94447 to 0.93330, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0915 - acc: 0.9861 - val_loss: 0.9333 - val_acc: 0.7059\n",
      "Epoch 46/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0692 - acc: 1.0000\n",
      "Epoch 00046: val_loss improved from 0.93330 to 0.92295, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0866 - acc: 0.9861 - val_loss: 0.9229 - val_acc: 0.7059\n",
      "Epoch 47/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0471 - acc: 1.0000\n",
      "Epoch 00047: val_loss improved from 0.92295 to 0.91563, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0817 - acc: 0.9861 - val_loss: 0.9156 - val_acc: 0.7059\n",
      "Epoch 48/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.1269 - acc: 0.9688\n",
      "Epoch 00048: val_loss improved from 0.91563 to 0.90822, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0779 - acc: 0.9861 - val_loss: 0.9082 - val_acc: 0.7059\n",
      "Epoch 49/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0401 - acc: 1.0000\n",
      "Epoch 00049: val_loss improved from 0.90822 to 0.90020, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0739 - acc: 0.9861 - val_loss: 0.9002 - val_acc: 0.7059\n",
      "Epoch 50/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.1073 - acc: 0.9688\n",
      "Epoch 00050: val_loss improved from 0.90020 to 0.88922, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0707 - acc: 0.9861 - val_loss: 0.8892 - val_acc: 0.7647\n",
      "Epoch 51/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.1043 - acc: 0.9688\n",
      "Epoch 00051: val_loss improved from 0.88922 to 0.87838, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0675 - acc: 0.9861 - val_loss: 0.8784 - val_acc: 0.7647\n",
      "Epoch 52/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0388 - acc: 1.0000\n",
      "Epoch 00052: val_loss improved from 0.87838 to 0.86858, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0643 - acc: 0.9861 - val_loss: 0.8686 - val_acc: 0.7647\n",
      "Epoch 53/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0930 - acc: 0.9688\n",
      "Epoch 00053: val_loss improved from 0.86858 to 0.85811, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0618 - acc: 0.9861 - val_loss: 0.8581 - val_acc: 0.7647\n",
      "Epoch 54/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0362 - acc: 1.0000\n",
      "Epoch 00054: val_loss improved from 0.85811 to 0.84909, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0592 - acc: 0.9861 - val_loss: 0.8491 - val_acc: 0.7647\n",
      "Epoch 55/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0324 - acc: 1.0000\n",
      "Epoch 00055: val_loss improved from 0.84909 to 0.84180, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0570 - acc: 0.9861 - val_loss: 0.8418 - val_acc: 0.7647\n",
      "Epoch 56/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0869 - acc: 0.9688\n",
      "Epoch 00056: val_loss improved from 0.84180 to 0.83504, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0551 - acc: 0.9861 - val_loss: 0.8350 - val_acc: 0.7647\n",
      "Epoch 57/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0258 - acc: 1.0000\n",
      "Epoch 00057: val_loss improved from 0.83504 to 0.82966, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0531 - acc: 0.9861 - val_loss: 0.8297 - val_acc: 0.7647\n",
      "Epoch 58/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0271 - acc: 1.0000\n",
      "Epoch 00058: val_loss improved from 0.82966 to 0.82312, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0514 - acc: 0.9861 - val_loss: 0.8231 - val_acc: 0.7647\n",
      "Epoch 59/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0832 - acc: 0.9688\n",
      "Epoch 00059: val_loss improved from 0.82312 to 0.81433, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0499 - acc: 0.9861 - val_loss: 0.8143 - val_acc: 0.7647\n",
      "Epoch 60/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0786 - acc: 0.9688\n",
      "Epoch 00060: val_loss improved from 0.81433 to 0.80692, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0479 - acc: 0.9861 - val_loss: 0.8069 - val_acc: 0.7647\n",
      "Epoch 61/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 00061: val_loss improved from 0.80692 to 0.79956, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0456 - acc: 0.9861 - val_loss: 0.7996 - val_acc: 0.7647\n",
      "Epoch 62/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0695 - acc: 0.9688\n",
      "Epoch 00062: val_loss improved from 0.79956 to 0.79078, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0443 - acc: 0.9861 - val_loss: 0.7908 - val_acc: 0.8235\n",
      "Epoch 63/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0664 - acc: 1.0000\n",
      "Epoch 00063: val_loss improved from 0.79078 to 0.78270, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0424 - acc: 1.0000 - val_loss: 0.7827 - val_acc: 0.8235\n",
      "Epoch 64/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0229 - acc: 1.0000\n",
      "Epoch 00064: val_loss improved from 0.78270 to 0.77624, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0405 - acc: 1.0000 - val_loss: 0.7762 - val_acc: 0.8235\n",
      "Epoch 65/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0621 - acc: 1.0000\n",
      "Epoch 00065: val_loss improved from 0.77624 to 0.77035, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0393 - acc: 1.0000 - val_loss: 0.7704 - val_acc: 0.8235\n",
      "Epoch 66/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 00066: val_loss improved from 0.77035 to 0.76573, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0376 - acc: 1.0000 - val_loss: 0.7657 - val_acc: 0.8235\n",
      "Epoch 67/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 00067: val_loss improved from 0.76573 to 0.76169, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0363 - acc: 1.0000 - val_loss: 0.7617 - val_acc: 0.8235\n",
      "Epoch 68/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 00068: val_loss improved from 0.76169 to 0.75783, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.0350 - acc: 1.0000 - val_loss: 0.7578 - val_acc: 0.8235\n",
      "Epoch 69/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0526 - acc: 1.0000\n",
      "Epoch 00069: val_loss improved from 0.75783 to 0.75333, saving model to model-v1.h5\n",
      "72/72 [==============================] - 1s 8ms/sample - loss: 0.0340 - acc: 1.0000 - val_loss: 0.7533 - val_acc: 0.8235\n",
      "Epoch 70/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0504 - acc: 1.0000\n",
      "Epoch 00070: val_loss improved from 0.75333 to 0.74830, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0325 - acc: 1.0000 - val_loss: 0.7483 - val_acc: 0.8235\n",
      "Epoch 71/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0175 - acc: 1.0000\n",
      "Epoch 00071: val_loss improved from 0.74830 to 0.74433, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.0309 - acc: 1.0000 - val_loss: 0.7443 - val_acc: 0.8235\n",
      "Epoch 72/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0485 - acc: 1.0000\n",
      "Epoch 00072: val_loss improved from 0.74433 to 0.74021, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.0299 - acc: 1.0000 - val_loss: 0.7402 - val_acc: 0.8235\n",
      "Epoch 73/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0473 - acc: 1.0000\n",
      "Epoch 00073: val_loss improved from 0.74021 to 0.73615, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.0288 - acc: 1.0000 - val_loss: 0.7361 - val_acc: 0.8235\n",
      "Epoch 74/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0433 - acc: 1.0000\n",
      "Epoch 00074: val_loss improved from 0.73615 to 0.73208, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0277 - acc: 1.0000 - val_loss: 0.7321 - val_acc: 0.8235\n",
      "Epoch 75/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0434 - acc: 1.0000\n",
      "Epoch 00075: val_loss improved from 0.73208 to 0.72822, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0267 - acc: 1.0000 - val_loss: 0.7282 - val_acc: 0.8235\n",
      "Epoch 76/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 00076: val_loss improved from 0.72822 to 0.72435, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0256 - acc: 1.0000 - val_loss: 0.7243 - val_acc: 0.8235\n",
      "Epoch 77/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0369 - acc: 1.0000\n",
      "Epoch 00077: val_loss improved from 0.72435 to 0.72105, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0249 - acc: 1.0000 - val_loss: 0.7210 - val_acc: 0.8235\n",
      "Epoch 78/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0381 - acc: 1.0000\n",
      "Epoch 00078: val_loss improved from 0.72105 to 0.71830, saving model to model-v1.h5\n",
      "72/72 [==============================] - 1s 8ms/sample - loss: 0.0240 - acc: 1.0000 - val_loss: 0.7183 - val_acc: 0.8235\n",
      "Epoch 79/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 00079: val_loss improved from 0.71830 to 0.71509, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0230 - acc: 1.0000 - val_loss: 0.7151 - val_acc: 0.8235\n",
      "Epoch 80/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0334 - acc: 1.0000\n",
      "Epoch 00080: val_loss improved from 0.71509 to 0.71090, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0224 - acc: 1.0000 - val_loss: 0.7109 - val_acc: 0.8235\n",
      "Epoch 81/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 00081: val_loss improved from 0.71090 to 0.70741, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0210 - acc: 1.0000 - val_loss: 0.7074 - val_acc: 0.8235\n",
      "Epoch 82/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0125 - acc: 1.0000\n",
      "Epoch 00082: val_loss improved from 0.70741 to 0.70493, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 6ms/sample - loss: 0.0202 - acc: 1.0000 - val_loss: 0.7049 - val_acc: 0.8235\n",
      "Epoch 83/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 00083: val_loss improved from 0.70493 to 0.70452, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0193 - acc: 1.0000 - val_loss: 0.7045 - val_acc: 0.8235\n",
      "Epoch 84/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0282 - acc: 1.0000\n",
      "Epoch 00084: val_loss improved from 0.70452 to 0.70428, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0187 - acc: 1.0000 - val_loss: 0.7043 - val_acc: 0.8235\n",
      "Epoch 85/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 00085: val_loss improved from 0.70428 to 0.70321, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0178 - acc: 1.0000 - val_loss: 0.7032 - val_acc: 0.8235\n",
      "Epoch 86/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 00086: val_loss improved from 0.70321 to 0.70193, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 6ms/sample - loss: 0.0172 - acc: 1.0000 - val_loss: 0.7019 - val_acc: 0.8235\n",
      "Epoch 87/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 00087: val_loss improved from 0.70193 to 0.70098, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0165 - acc: 1.0000 - val_loss: 0.7010 - val_acc: 0.8235\n",
      "Epoch 88/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 00088: val_loss improved from 0.70098 to 0.70073, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0159 - acc: 1.0000 - val_loss: 0.7007 - val_acc: 0.8235\n",
      "Epoch 89/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 00089: val_loss improved from 0.70073 to 0.69959, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0154 - acc: 1.0000 - val_loss: 0.6996 - val_acc: 0.8235\n",
      "Epoch 90/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 00090: val_loss improved from 0.69959 to 0.69742, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0148 - acc: 1.0000 - val_loss: 0.6974 - val_acc: 0.8235\n",
      "Epoch 91/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 00091: val_loss improved from 0.69742 to 0.69684, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.0143 - acc: 1.0000 - val_loss: 0.6968 - val_acc: 0.8235\n",
      "Epoch 92/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 00092: val_loss improved from 0.69684 to 0.69612, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0138 - acc: 1.0000 - val_loss: 0.6961 - val_acc: 0.8235\n",
      "Epoch 93/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0180 - acc: 1.0000\n",
      "Epoch 00093: val_loss improved from 0.69612 to 0.69567, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0133 - acc: 1.0000 - val_loss: 0.6957 - val_acc: 0.8235\n",
      "Epoch 94/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0179 - acc: 1.0000\n",
      "Epoch 00094: val_loss improved from 0.69567 to 0.69452, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0129 - acc: 1.0000 - val_loss: 0.6945 - val_acc: 0.8235\n",
      "Epoch 95/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 00095: val_loss improved from 0.69452 to 0.69332, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0125 - acc: 1.0000 - val_loss: 0.6933 - val_acc: 0.8235\n",
      "Epoch 96/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 00096: val_loss improved from 0.69332 to 0.69257, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0121 - acc: 1.0000 - val_loss: 0.6926 - val_acc: 0.8235\n",
      "Epoch 97/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0166 - acc: 1.0000\n",
      "Epoch 00097: val_loss did not improve from 0.69257\n",
      "72/72 [==============================] - 0s 597us/sample - loss: 0.0118 - acc: 1.0000 - val_loss: 0.6926 - val_acc: 0.8235\n",
      "Epoch 98/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 00098: val_loss improved from 0.69257 to 0.69210, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0113 - acc: 1.0000 - val_loss: 0.6921 - val_acc: 0.8235\n",
      "Epoch 99/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 00099: val_loss improved from 0.69210 to 0.69112, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0110 - acc: 1.0000 - val_loss: 0.6911 - val_acc: 0.8235\n",
      "Epoch 100/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 00100: val_loss improved from 0.69112 to 0.69008, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0107 - acc: 1.0000 - val_loss: 0.6901 - val_acc: 0.8235\n",
      "Epoch 101/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 00101: val_loss improved from 0.69008 to 0.68949, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0103 - acc: 1.0000 - val_loss: 0.6895 - val_acc: 0.8235\n",
      "Epoch 102/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 0.68949\n",
      "72/72 [==============================] - 0s 722us/sample - loss: 0.0101 - acc: 1.0000 - val_loss: 0.6896 - val_acc: 0.8235\n",
      "Epoch 103/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 00103: val_loss improved from 0.68949 to 0.68936, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0097 - acc: 1.0000 - val_loss: 0.6894 - val_acc: 0.8235\n",
      "Epoch 104/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 00104: val_loss improved from 0.68936 to 0.68890, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0095 - acc: 1.0000 - val_loss: 0.6889 - val_acc: 0.8235\n",
      "Epoch 105/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 00105: val_loss improved from 0.68890 to 0.68746, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0092 - acc: 1.0000 - val_loss: 0.6875 - val_acc: 0.8235\n",
      "Epoch 106/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 00106: val_loss improved from 0.68746 to 0.68583, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0090 - acc: 1.0000 - val_loss: 0.6858 - val_acc: 0.8235\n",
      "Epoch 107/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 00107: val_loss did not improve from 0.68583\n",
      "72/72 [==============================] - 0s 625us/sample - loss: 0.0087 - acc: 1.0000 - val_loss: 0.6865 - val_acc: 0.8235\n",
      "Epoch 108/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.68583\n",
      "72/72 [==============================] - 0s 556us/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 0.6861 - val_acc: 0.8235\n",
      "Epoch 109/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 00109: val_loss improved from 0.68583 to 0.68566, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0083 - acc: 1.0000 - val_loss: 0.6857 - val_acc: 0.8235\n",
      "Epoch 110/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 00110: val_loss improved from 0.68566 to 0.68528, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0081 - acc: 1.0000 - val_loss: 0.6853 - val_acc: 0.8235\n",
      "Epoch 111/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 00111: val_loss improved from 0.68528 to 0.68425, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0079 - acc: 1.0000 - val_loss: 0.6843 - val_acc: 0.8235\n",
      "Epoch 112/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 00112: val_loss improved from 0.68425 to 0.68298, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0077 - acc: 1.0000 - val_loss: 0.6830 - val_acc: 0.8235\n",
      "Epoch 113/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 00113: val_loss improved from 0.68298 to 0.68014, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0075 - acc: 1.0000 - val_loss: 0.6801 - val_acc: 0.8235\n",
      "Epoch 114/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 00114: val_loss improved from 0.68014 to 0.67782, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0073 - acc: 1.0000 - val_loss: 0.6778 - val_acc: 0.8235\n",
      "Epoch 115/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 00115: val_loss improved from 0.67782 to 0.67596, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0072 - acc: 1.0000 - val_loss: 0.6760 - val_acc: 0.8235\n",
      "Epoch 116/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 00116: val_loss improved from 0.67596 to 0.67501, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0070 - acc: 1.0000 - val_loss: 0.6750 - val_acc: 0.8235\n",
      "Epoch 117/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 00117: val_loss improved from 0.67501 to 0.67468, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0068 - acc: 1.0000 - val_loss: 0.6747 - val_acc: 0.8235\n",
      "Epoch 118/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00118: val_loss improved from 0.67468 to 0.67463, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0067 - acc: 1.0000 - val_loss: 0.6746 - val_acc: 0.8235\n",
      "Epoch 119/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 0.67463\n",
      "72/72 [==============================] - 0s 597us/sample - loss: 0.0065 - acc: 1.0000 - val_loss: 0.6748 - val_acc: 0.8235\n",
      "Epoch 120/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 00120: val_loss improved from 0.67463 to 0.67461, saving model to model-v1.h5\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0064 - acc: 1.0000 - val_loss: 0.6746 - val_acc: 0.8235\n",
      "Epoch 121/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 00121: val_loss improved from 0.67461 to 0.67448, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0063 - acc: 1.0000 - val_loss: 0.6745 - val_acc: 0.8235\n",
      "Epoch 122/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 00122: val_loss improved from 0.67448 to 0.67439, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0063 - acc: 1.0000 - val_loss: 0.6744 - val_acc: 0.8235\n",
      "Epoch 123/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 00123: val_loss improved from 0.67439 to 0.67427, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.6743 - val_acc: 0.8235\n",
      "Epoch 124/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 00124: val_loss improved from 0.67427 to 0.67426, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.6743 - val_acc: 0.8235\n",
      "Epoch 125/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 00125: val_loss improved from 0.67426 to 0.67411, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.6741 - val_acc: 0.8235\n",
      "Epoch 126/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 00126: val_loss improved from 0.67411 to 0.67379, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.6738 - val_acc: 0.8235\n",
      "Epoch 127/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 00127: val_loss improved from 0.67379 to 0.67336, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.6734 - val_acc: 0.8235\n",
      "Epoch 128/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 00128: val_loss improved from 0.67336 to 0.67300, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.6730 - val_acc: 0.8235\n",
      "Epoch 129/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 00129: val_loss improved from 0.67300 to 0.67274, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.6727 - val_acc: 0.8235\n",
      "Epoch 130/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 00130: val_loss improved from 0.67274 to 0.67244, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 2ms/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.6724 - val_acc: 0.8235\n",
      "Epoch 131/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 00131: val_loss improved from 0.67244 to 0.67211, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.6721 - val_acc: 0.8235\n",
      "Epoch 132/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 00132: val_loss improved from 0.67211 to 0.67199, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.6720 - val_acc: 0.8235\n",
      "Epoch 133/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 00133: val_loss improved from 0.67199 to 0.67186, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.6719 - val_acc: 0.8235\n",
      "Epoch 134/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 00134: val_loss improved from 0.67186 to 0.67165, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.6716 - val_acc: 0.8235\n",
      "Epoch 135/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 00135: val_loss improved from 0.67165 to 0.67125, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.6713 - val_acc: 0.8235\n",
      "Epoch 136/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 00136: val_loss improved from 0.67125 to 0.67086, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.6709 - val_acc: 0.8235\n",
      "Epoch 137/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 00137: val_loss improved from 0.67086 to 0.67046, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.6705 - val_acc: 0.8235\n",
      "Epoch 138/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 00138: val_loss improved from 0.67046 to 0.67003, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.6700 - val_acc: 0.8235\n",
      "Epoch 139/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 00139: val_loss improved from 0.67003 to 0.66976, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.6698 - val_acc: 0.8235\n",
      "Epoch 140/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 00140: val_loss improved from 0.66976 to 0.66960, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.6696 - val_acc: 0.8235\n",
      "Epoch 141/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 00141: val_loss improved from 0.66960 to 0.66934, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.6693 - val_acc: 0.8235\n",
      "Epoch 142/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00142: val_loss improved from 0.66934 to 0.66910, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.6691 - val_acc: 0.8235\n",
      "Epoch 143/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 00143: val_loss improved from 0.66910 to 0.66881, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.6688 - val_acc: 0.8235\n",
      "Epoch 144/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 00144: val_loss improved from 0.66881 to 0.66835, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.6684 - val_acc: 0.8235\n",
      "Epoch 145/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 00145: val_loss improved from 0.66835 to 0.66804, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.6680 - val_acc: 0.8235\n",
      "Epoch 146/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00146: val_loss improved from 0.66804 to 0.66761, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.6676 - val_acc: 0.8235\n",
      "Epoch 147/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 00147: val_loss improved from 0.66761 to 0.66701, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 5ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.6670 - val_acc: 0.8235\n",
      "Epoch 148/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 00148: val_loss improved from 0.66701 to 0.66643, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0056 - acc: 1.0000 - val_loss: 0.6664 - val_acc: 0.8235\n",
      "Epoch 149/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 00149: val_loss improved from 0.66643 to 0.66601, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0056 - acc: 1.0000 - val_loss: 0.6660 - val_acc: 0.8235\n",
      "Epoch 150/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 00150: val_loss improved from 0.66601 to 0.66564, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0056 - acc: 1.0000 - val_loss: 0.6656 - val_acc: 0.8235\n",
      "Epoch 151/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 00151: val_loss improved from 0.66564 to 0.66547, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0056 - acc: 1.0000 - val_loss: 0.6655 - val_acc: 0.8235\n",
      "Epoch 152/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 00152: val_loss did not improve from 0.66547\n",
      "72/72 [==============================] - 0s 653us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6656 - val_acc: 0.8235\n",
      "Epoch 153/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.66547\n",
      "72/72 [==============================] - 0s 764us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6656 - val_acc: 0.8235\n",
      "Epoch 154/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.66547\n",
      "\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "72/72 [==============================] - 0s 667us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6655 - val_acc: 0.8235\n",
      "Epoch 155/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 00155: val_loss did not improve from 0.66547\n",
      "72/72 [==============================] - 0s 583us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6655 - val_acc: 0.8235\n",
      "Epoch 156/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 00156: val_loss did not improve from 0.66547\n",
      "72/72 [==============================] - 0s 792us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6655 - val_acc: 0.8235\n",
      "Epoch 157/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 00157: val_loss improved from 0.66547 to 0.66545, saving model to model-v1.h5\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "72/72 [==============================] - 0s 4ms/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6655 - val_acc: 0.8235\n",
      "Epoch 158/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 00158: val_loss improved from 0.66545 to 0.66545, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 159/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 00159: val_loss improved from 0.66545 to 0.66545, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 160/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00160: val_loss did not improve from 0.66545\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "72/72 [==============================] - 0s 820us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 161/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.66545\n",
      "72/72 [==============================] - 0s 625us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 162/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.66545\n",
      "72/72 [==============================] - 0s 653us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 163/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00163: val_loss improved from 0.66545 to 0.66544, saving model to model-v1.h5\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 164/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 00164: val_loss improved from 0.66544 to 0.66544, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 165/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 00165: val_loss improved from 0.66544 to 0.66544, saving model to model-v1.h5\n",
      "72/72 [==============================] - 0s 3ms/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 166/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.66544\n",
      "\n",
      "Epoch 00166: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "72/72 [==============================] - 0s 889us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 167/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 00167: val_loss did not improve from 0.66544\n",
      "72/72 [==============================] - 0s 653us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 168/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 00168: val_loss did not improve from 0.66544\n",
      "72/72 [==============================] - 0s 599us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 169/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 00169: val_loss did not improve from 0.66544\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "72/72 [==============================] - 0s 653us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 170/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 00170: val_loss did not improve from 0.66544\n",
      "72/72 [==============================] - 0s 722us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 171/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00171: val_loss did not improve from 0.66544\n",
      "72/72 [==============================] - 0s 667us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 172/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 00172: val_loss did not improve from 0.66544\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "72/72 [==============================] - 0s 694us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 173/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 00173: val_loss did not improve from 0.66544\n",
      "72/72 [==============================] - 0s 639us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 174/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 00174: val_loss did not improve from 0.66544\n",
      "72/72 [==============================] - 0s 667us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n",
      "Epoch 175/500\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.66544\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "72/72 [==============================] - 0s 695us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8235\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=500, verbose=1,validation_data=(X_test,y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6UAAAHiCAYAAAAQ42q7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzs3Xl83WWZ9/HPlX1P2pN0SdM2XWjpnoZS9s2FpQJ1kAFRUHEYRB2XQXym+lIfx2FmGOVB3LcZUGdYZFAElWVGBQFRoK2lBUpp6ZqkaZO0zXKyJ/fzx+8kTdMs5yTnnN9Jzvf9euV1tt9y5Zy2v17nvu7rNuccIiIiIiIiIn5I8TsAERERERERSV5KSkVERERERMQ3SkpFRERERETEN0pKRURERERExDdKSkVERERERMQ3SkpFRERERETEN0pKJebMLNXMWsxsTjS39ZOZLTSzmKynNPjYZvY/Zvb+WMRhZl80s++PdX8REUkuuqaP79i6posMTUmpnCR0Aen76TWztgGPh/yHdCTOuR7nXJ5zbn80t01UZvY7M/vSEM+/x8yqzSyiv3fOuYudc/dFIa53mNneQcf+J+fcLeM99ijndGZ2a6zOISIiw9M1fXx0TQczu8nMnon2cUUGUlIqJwldQPKcc3nAfuCKAc+d9A+pmaXFP8qE9mPghiGevwH4L+dcb3zD8dUHgSOh27jSn0sREV3To+DH6JouEnNKSiViZna7mf3MzB4ws2bgejM7y8z+bGbHzOygmX3TzNJD26eFRsvKQ4//K/T6E2bWbGZ/MrN5kW4bev0yM3vTzBrN7Ftm9kcz+9AwcYcT40fMbJeZHTWzbw7YN9XMvm5mDWb2FnDpCG/RL4AZZnb2gP0DwDrgp6HHV5rZltDvtN/MvjjC+/183+80WhyhbzO3h477lpndFHq+EPgVMGfAN+TTQp/ljwfs/24zey30Hv3ezBYPeK3KzG41s22h9/sBM8scIe484Crgo8BSM6sY9Pr5oc+j0cwOmNkNoedzQr/j/tBrz5pZ5lDfCodiujB0P6I/l6F9VpjZb83siJnVmtn/MbNZZtZqZkUDtjsj9Lr+syYik4qu6bqmh3NNH+H3KTOzX4euozvN7MMDXjvTzDabWZOZHTKzr4WezzGz+0O/9zEze8nMiiM9t0wuSkplrP4KuB8oBH4GdAOfAoqBc/D+Yf3ICPu/D/giMBXvm9t/inRbM5sGPAR8NnTePcDaEY4TTozrgNOA1XgX5neEnv8ocDGwKnSOa4Y7iXMuCDwMfGDA0+8FtjrnXgs9bgGux3v/rgA+ZWaXjxB7n9HiOAS8CygA/hb4lpmtdM41hs6zf8A35IcH7mhmS4D/Aj4BlAC/BX41MIkLne+dwHy892mob4/7/DVwFO+9+C0D3o/Qf0J+A9wFBPDe722hl78OrATOwPvMPw+E+0102H8uQxf13+Jd2GcCi4BnnHPVwPOh+PtcDzzgnOsOMw4RkYlE1/Rh6Jo+qp/hfValwLXAV83sgtBr3wK+5pwrABbivY8ANwI5QBne/wE+BrSP4dwyiSgplbF63jn3K+dcr3OuzTn3snPuRedct3NuN/BD4IIR9n/YObfROdcF3AdUjGHby4EtzrlHQ699Hagf7iBhxvivzrlG59xe4JkB57oG+Lpzrso51wDcMUK8AD8BrhnwreMHQs/1xfJ759yroffvFeDBIWIZyohxhD6T3c7ze+B3wHlhHBe8i+xjodi6QscuwEsO+9ztnKsNnfvXjPy5fRB4MFTadD/w/gEjjdcDTzrnHgp9HvXOuS1mlgp8CPikc+5gaD7S86F4whHJn8srgQPOuW845zqcc03OuZdCr/0kFGNfKdu1wH+GGYOIyESja/rIdE0fQugL5rXABudcu3NuM3Avx5PbLuAUMws455qdcy8OeL4YWBi6zm90zrVEcm6ZfJSUylgdGPjAzE41s9+EShybgK/g/YMznNoB91uBvDFsWzowDuecA6qGO0iYMYZ1LmDfCPEC/AFoBK4ws0V439I+MCCWs8zsGTOrM7NG4KYhYhnKiHGY2eVm9mKojOYY3jew4ZbElA48XiiZrAJmDdgmrM8tVKp1Pt5/OAAeCW3bV5o0G3hriF2nAxnDvBaOSP5czgZ2DXOcR4BV5nWMvBSoC11sRUQmI13TR5bU1/RRzlEfGk3us2/AOW4ElgI7QiW660LP/xhv5PYh85pF3WGaHpP0lJTKWA1uWf4D4FW8b70KgC8BFuMYDuKVfgBgZsaJ/9gONp4YD+IlMX1GbG8fupj+J963qTcAjzvnBn7j+yDwc2C2c64Q+PcwYxk2DjPLxiuN+VdgunOuCPifAccdrc18DTB3wPFS8N7f6jDiGuwDofM+YWa1eMlfBsfLnw4AC4bY7xDQOcxrQbxyn7740vDKfgaK5M/lcDHgnGvF+3zej/f5aZRURCYzXdNHoGv6iOcoNrPcAc/N6TuHc26Hc+69wDTg/wE/N7Ms51ync+7LzrklwLl45eMRd4KWyUVJqURLPt63iMHQPIaR5p5Ey6+BSjO7IpSgfApv3kQsYnwI+LR5TXACwD+Esc9P8EbZPsyAMp8BsRxxzrWb2Zl4ZTbjjSMTL/GrA3pC81nePuD1Q3gXj/wRjn2lmV0YmnPyWaAZeHGY7UfyAbz/IFQM+Lk2dPwpePNcLjWvpX6amRWb2SrnXA/eN6h3m9kM85pAnBOK5w0g38wuCT3+v0D6EOceaKTP/DG8JhF/Z2YZZlZgZgPnL/0U77N7VyheEZFkoWv6yZL5mg6QYmZZA3+cc3uAjcC/mNeQsAJvdPQ+ADO7wcyKQ6O0jXiJdK+Zvc3MlocS5Sa8ct6eMcYlk4SSUomWz+DNIWzG+/byZ7E+oXPuEF6icxfQgDfq9RegIwYxfg9vLsc24GWOT9YfKb63gJeALLymPgN9FPhX8zodfh7v4jGuOJxzx4C/xys9PQJcjXeR73v9Vbxvcvea1+1u2qB4X8N7f76HdxG8FLgygvmcAJjZuXglPd8JzVWpdc7VhuLaC1wbupBdgXcBPgJsBlaEDvH3wHZgU+i1fwHMOXcUr2HDT/C+hT3CiaVHQxn2M3deo4h3Au8BDgNvcuIcoGeBVOBF59ywJWQiIpOQruknx5eU1/QBzgPaBv2A95mdgnc9fhj4vHPu6dBr64DtofflTrzrfyfe/xF+gZeQvoZXyttfDi3JybyKBJGJL9Qkpwa42jn3nN/xyMRnZs8C9zjnfux3LCIiyUTXdJHkopFSmdDM7FIzKwx1xPsiXov4l0bZTWRUoRKs5cB/+x2LiEgy0DVdJHkpKZWJ7lxgN17b+EuBdzvnhiv1EQmLmd0HPAl8alBXQRERiR1d00WSlMp3RURERERExDcaKRURERERERHfKCkVERERERER36T5deLi4mJXXl7u1+lFRGSS2bRpU71zbqR1DWUUujaLiEg0hXtt9i0pLS8vZ+PGjX6dXkREJhkz2+d3DBOdrs0iIhJN4V6bVb4rIiIiIiIivlFSKiIiIiIiIr5RUioiIiIiIiK+8W1OqYiIiIiISJ+uri6qqqpob2/3OxSJUFZWFmVlZaSnp49pfyWlIiIiIiLiu6qqKvLz8ykvL8fM/A5HwuSco6GhgaqqKubNmzemY6h8V0REREREfNfe3k4gEFBCOsGYGYFAYFwj3EpKRUREREQkISghnZjG+7kpKRURERERkaTX0NBARUUFFRUVzJgxg1mzZvU/7uzsDOsYN954Izt27Bhxm+985zvcd9990QiZc889ly1btkTlWH7SnFIREREREUl6gUCgP8H78pe/TF5eHrfddtsJ2zjncM6RkjL02N6999476nk+/vGPjz/YSUYjpSIiIiIiIsPYtWsXy5cv55ZbbqGyspKDBw9y8803s2bNGpYtW8ZXvvKV/m37Ri67u7spKipiw4YNrFq1irPOOovDhw8D8IUvfIG77767f/sNGzawdu1aFi9ezAsvvABAMBjkPe95D6tWreK6665jzZo1YY+ItrW18cEPfpAVK1ZQWVnJs88+C8C2bds4/fTTqaioYOXKlezevZvm5mYuu+wyVq1axfLly3n44Yej+daFTSOlIiIiIiKSUP7xV6/xek1TVI+5tLSA/3vFsjHt+/rrr3Pvvffy/e9/H4A77riDqVOn0t3dzUUXXcTVV1/N0qVLT9insbGRCy64gDvuuINbb72Ve+65hw0bNpx0bOccL730Eo899hhf+cpXePLJJ/nWt77FjBkz+PnPf84rr7xCZWVl2LF+85vfJCMjg23btvHaa6+xbt06du7cyXe/+11uu+02rr32Wjo6OnDO8eijj1JeXs4TTzzRH7MfNFIqIiIiIiIyggULFnD66af3P37ggQeorKyksrKS7du38/rrr5+0T3Z2NpdddhkAp512Gnv37h3y2FddddVJ2zz//PO8973vBWDVqlUsWxZ+Mv38889zww03ALBs2TJKS0vZtWsXZ599Nrfffjtf/epXOXDgAFlZWaxcuZInn3ySDRs28Mc//pHCwsKwzxNNGikVEREREZGEMtYRzVjJzc3tv79z506+8Y1v8NJLL1FUVMT1118/5HIoGRkZ/fdTU1Pp7u4e8tiZmZknbeOcG3Osw+17ww03cNZZZ/Gb3/yGd77znfzkJz/h/PPPZ+PGjTz++ON89rOf5fLLL+fzn//8mM89VhopFRERERERCVNTUxP5+fkUFBRw8OBBnnrqqaif49xzz+Whhx4CvLmgQ43EDuf888/v7+67fft2Dh48yMKFC9m9ezcLFy7kU5/6FO9617vYunUr1dXV5OXlccMNN3DrrbeyefPmqP8u4Rh1pNTM7gEuBw4755YP8boB3wDWAa3Ah5xz/vw2IiIiIiIiMVRZWcnSpUtZvnw58+fP55xzzon6OT7xiU/wgQ98gJUrV1JZWcny5cuHLa295JJLSE9PB+C8887jnnvu4SMf+QgrVqwgPT2dn/70p2RkZHD//ffzwAMPkJ6eTmlpKbfffjsvvPACGzZsICUlhYyMjP45s/Fmow0Nm9n5QAvw02GS0nXAJ/CS0jOAbzjnzhjtxGvWrHEbN24cU9AiIiKDmdkm59wav+OYyHRtFhE/bd++nSVLlvgdRkLo7u6mu7ubrKwsdu7cycUXX8zOnTtJS0vc2ZdDfX7hXptH/a2cc8+aWfkIm6zHS1gd8GczKzKzmc65g6MdWySaensdzR1D1+qLSOLKzUglLVWzSSaTo8FOcjPTyEjT5yoiMhYtLS28/e1vp7u7G+ccP/jBDxI6IR2vaPxms4ADAx5XhZ5TUipx9eGfvMwzO+r8DkNEIvTrT5zL8ln+dPuT6Hv2zTo+cM9LPHzLWawpn+p3OCIiE1JRURGbNm3yO4y4iUZSakM8N2RNsJndDNwMMGfOnCicWsRTfayNZ3bUsW7FDE6bq/8EiUwkMwqz/A5BomjBtDwA3qhtVlIqIiJhiUZSWgXMHvC4DKgZakPn3A+BH4I3byUK5xYB4LEt3h+5DZcuYU4gx+doRESSV2lhFvmZabx5qNnvUEREZIKIxmSPx4APmOdMoFHzSSXeHt1Szeo5RUpIRUR8ZmYsmpHPG7VKSkVEJDyjJqVm9gDwJ2CxmVWZ2d+Y2S1mdktok8eB3cAu4EfAx2IWrcgQdtQ280ZtM++umOV3KCIi4hzrct5gx8GmcS3+LiIiySOc7rvXjfK6Az4etYgkbM45jrV2+R2G7x7edIDUFONdK2f6HYqIiOx4gr/Z8/f8pfMTHG6+gOkFmjMsIhPDhRdeyOc+9zkuueSS/ufuvvtu3nzzTb773e8Ou19eXh4tLS3U1NTwyU9+kocffnjIY995552sWTP86ih33303N998Mzk5XuXfunXruP/++ykqKhrHbwVf/vKXycvL47bbbhvXcWJp8vYVTgJf+fXr3PvHvX6HkRAuWFRCcV6m32GIiMiiSwhOWcI/NDzIjupbmF5Q5ndEIiJhue6663jwwQdPSEoffPBBvva1r4W1f2lp6ZAJabjuvvturr/++v6k9PHHHx/zsSYaJaUTVEd3Dz/fVMWZ86dy6bIZfofju7cvme53CCIiApCSSs87/onZ/301NRt/BEv+0e+IRETCcvXVV/OFL3yBjo4OMjMz2bt3LzU1NZx77rm0tLSwfv16jh49SldXF7fffjvr168/Yf+9e/dy+eWX8+qrr9LW1saNN97I66+/zpIlS2hra+vf7qMf/Sgvv/wybW1tXH311fzjP/4j3/zmN6mpqeGiiy6iuLiYp59+mvLycjZu3EhxcTF33XUX99xzDwA33XQTn/70p9m7dy+XXXYZ5557Li+88AKzZs3i0UcfJTs7O6zfd6hjBoNBrrnmGqqqqujp6eGLX/wi1157LRs2bOCxxx4jLS2Niy++mDvvvDNK77pHSekE9cyOOprau/nohQu5YFGJ3+GIiIj0K1j2Tp5/uJLT9vw7BD8JuQG/QxKRieaJDVC7LbrHnLECLrtj2JcDgQBr167lySefZP369Tz44INce+21mBlZWVk88sgjFBQUUF9fz5lnnsmVV16J2VCrY8L3vvc9cnJy2Lp1K1u3bqWysrL/tX/+539m6tSp9PT08Pa3v52tW7fyyU9+krvuuounn36a4uLiE461adMm7r33Xl588UWcc5xxxhlccMEFTJkyhZ07d/LAAw/wox/9iGuuuYaf//znXH/99aO+FcMdc/fu3ZSWlvKb3/wGgMbGRo4cOcIjjzzCG2+8gZlx7NixcN7tiESj+6744NEt1RTnZXDOAl3oRUQk8fxq+kfJ6G2FP/yb36GIiIStr4QXvNLd667z2us45/j85z/PypUrecc73kF1dTWHDh0a9jjPPvtsf3K4cuVKVq5c2f/aQw89RGVlJatXr+a1117j9ddfHzGm559/nr/6q78iNzeXvLw8rrrqKp577jkA5s2bR0VFBQCnnXYae/fuDev3HO6YK1as4Le//S3/8A//wHPPPUdhYSEFBQVkZWVx00038Ytf/KK/vDiaNFI6ATW3d/Hb7Yd539o5pKXqewUREUk8+WXL+e+at3Htxv/A1t4MxQv9DklEJpIRRjRj6d3vfje33normzdvpq2trX+E87777qOuro5NmzaRnp5OeXk57e3tIx5rqFHUPXv2cOedd/Lyyy8zZcoUPvShD416nJE6mWdmHu+pkpqaekKZ8FiOuWjRIjZt2sTjjz/O5z73OS6++GK+9KUv8dJLL/G73/2OBx98kG9/+9v8/ve/D+s84VJG4xPnHEeCnTS0dET888stNXR297K+otTvX0NERGRIi2bkc2fne3CpmfC/X/I7HBGRsOTl5XHhhRfy4Q9/uH+UFLwy1mnTppGens7TTz/Nvn37RjzO+eefz3333QfAq6++ytatWwFoamoiNzeXwsJCDh06xBNPPNG/T35+Ps3NJ6/xfP755/PLX/6S1tZWgsEgjzzyCOedd964fs/hjllTU0NOTg7XX389t912G5s3b6alpYXGxkbWrVvH3XffzZYtW8Z17qFopNQn3/79Lv7f/7455v3nBnKomD2+9tAiIiKxcuqMfOopZOeiv2Xxa3fD3ueh/Fy/wxIRGdV1113HVVdd1V/GC/D+97+fK664gjVr1lBRUcGpp5464jE++tGPcuONN7Jy5UoqKipYu3YtAKtWrWL16tUsW7aM+fPnc8455/Tvc/PNN3PZZZcxc+ZMnn766f7nKysr+dCHPtR/jJtuuonVq1eHXaoLcPvtt3P33Xf3P66qqhrymE899RSf/exnSUlJIT09ne9973s0Nzezfv162tvbcc7x9a9/Pezzhsv8Wth6zZo1buPGjb6cOxF8/P7NvLTnCJ9429jKmU6bO4VlpYVRjkpEZOIys03OueEXgJNRRfPa3N7Vw8ov/w83nTWD/7PjOpi6AG78TVSOLSKT0/bt21myZInfYcgYDfX5hXtt1kipT+qbO5gXyOUDZ5X7HYqIiEjUZaWnsrKskBf2tcI5n4InN8C+F2Du2X6HJiIiCUZzSn1S39JBIC/D7zBERERi5vR5U3m1upG2FddDbgn84at+hyQiIglISalPGoKdFOdljr6hiIjIBHV6+RS6ex1bajvhrL+D3U9DVfJO3RERkaEpKfVBV08vx1q7lJSKiMikdtqcqZjBy3uPwOl/A9lT4Nmv+R2WiCQwv/rdyPiM93NTUuqDhpZOAJXviojIpFaYk87i6fleUpqZD2d+DN58Eg6+4ndoIpKAsrKyaGhoUGI6wTjnaGhoICsra8zHUKMjH9S3dABopFRERCa9NeVTeGRzNd09vaStvRle+BY8eydc+59+hyYiCaasrIyqqirq6ur8DkUilJWVRVlZ2Zj3V1Lqg76ktCRfI6UiIjK5nV4+lf/6837eqG1m+awiWHszPHcnHN4O07T0g4gcl56ezrx58/wOQ3yg8l0f1PeV7+ZqpFRERCa308unAvDSniPeE2d+DNJz4bm7fIxKREQSiZJSHzT0le/mKykVEZHJrbQom7Ip2ceT0twAnPZBeO0X0HTQ3+BERCQhKCn1QX1LB1npKeRmpPodioiISMydNT/An/c00Nsbal5y+k3Q2wMb7/E3MBERSQhKSn1Q3+KtUWpmfociIiISc2fOD3CstYsdh5q9JwILYNElsOle6O7wNzgREfGdklIf1Ld0EFDnXRERSRJnLggA8Ke3Go4/ecZHIFgHrz3iU1QiIpIolJT6oL6lkxKtUSoiIkliVlE2c6bm8OfdA5LS+RdB8WL48/dAaxKKiCQ1JaU+qG/p0BqlIiKSVM6cP5UX9xw5Pq/UDM64GQ5ugaqX/Q1ORER8paQ0znp7HUeCnQQ0UioiIknkzPkBGtu62F7bdPzJle+FzEJ48fv+BSYiIr5TUhpnx9q66Ol1GikVEZGkcuZ8b17pn3cfOf5kZh6svh5efxSaanyKTERE/KakNM7q+9YoVVIqIiJJpLQom7mBQfNKAdZqeRgRkWSnpDTO6pu9pFTluyIikmzOnBfgxd0N9PQOaGw0dT4suhQ23gvdnf4FJyIivlFSGmf1Qe+CW6KRUhERSTJnLQjQ1N7N9oNNJ76w5kZorYed/+NPYCIi4islpXHWN1Kq8l0REUk2Z8yfCnByCe+Ct0PuNNhyvw9RiYiI35SUxll9SwepKUZhdrrfoYiIiMTVzMJsyoeaV5qaBquuhZ1PQUudP8GJiIhvlJTGSXtXD1VHWzlwtI1AbgYpKeZ3SCIiInF31oIAL+45cuK8UoCK90NvN2z7b38CExER3ygpjQPnHH/9/T9x7r89za9eqWFGYZbfIYmIiPjizPkBmtu7eb1m0LzSaUugtFIlvCIiSSjN7wCSwRu1zWyrbuR9Z8yhYnYRFbOL/A5JRETEF8fXK21gRVnhiS9WvA8evw1qX4UZy32ITkRE/KCR0jj45ZZq0lKM2y5ezDVrZrNoer7fIYmIiPhiekEW84tzT55XCrDsKkhJUwmviEiSUVIaY729jl9tqeH8RSVMzdXapCIiImvKp7Bp/1F6B88rzQ3AgrfBqz+H3l5/ghMRkbhTUhpjL+89Qk1jO+srSv0ORUREJCGsmTuVY61d7K4Pnvziir+GxgNw4MX4ByYiIr5QUhpl9S0d7G9o7f/52cYD5GSk8s6l0/0OTUREJCFUzp0CwKZ9R05+cfE6SMtWCa+ISBJRo6MoeqO2iXXfeI7B1UjvriglJ0NvtYiICMCCklyKctLZtO8o154+58QXM/Pg1HXw2iNw2b9Bqtb1FhGZ7JQpRdEvNleTYsYdV60gNbQOqRmcv6jE58hEREQSh5lx2pwpbNp3dOgNll/tzSvd8wdY+I74BiciInGnpDRKenodj22p4cLFJVxz+my/wxERkUnMzGYDPwVmAL3AD51z3xi0zYXAo8Ce0FO/cM59JZ5xjqRy7hR+98ZhjgY7mTK4EeCCiyA9B3Y8qaRURCQJaE5plLy4p4HapnbWV8zyOxQREZn8uoHPOOeWAGcCHzezpUNs95xzriL0kzAJKcCa0LzSzfuHGC1Nz4b5F8GOJ8C5k18XEZFJRUlplDy2pYbcjFTesUQNjUREJLaccwedc5tD95uB7cCE+lZ0ZVkRaSk2fAnv4sugqQpqt8U3MBERiTslpVHQ0d3D49sOcsmyGWRnpPodjoiIJBEzKwdWA0OtoXKWmb1iZk+Y2bK4BjaK7IxUlpUWsHG4pHTRJYDBm0/GNS4REYk/JaVR8MyOOprau1m/ekJ9SS0iIhOcmeUBPwc+7ZxrGvTyZmCuc24V8C3gl8Mc42Yz22hmG+vq6mIb8CCr50xhW1Uj3T29J7+YNw3K1sCOx+Mak4iIxJ+S0ih4dEs1xXkZnLMg4HcoIiKSJMwsHS8hvc8594vBrzvnmpxzLaH7jwPpZlY8xHY/dM6tcc6tKSmJb7f41XOKaOvqYceh5qE3WHwZ1PwFmg7GNS4REYkvJaXj1NTexW+3H+bylaWkpertFBGR2DMzA/4D2O6cu2uYbWaEtsPM1uJd8xviF+XoKmYXAbDlwLGhN1i8zrtVCa+IyKSmLGqcnnq1ls7uXtZXlPodioiIJI9zgBuAt5nZltDPOjO7xcxuCW1zNfCqmb0CfBN4r3OJ1cp2ztQcpuZmsGX/MElpyalQNNfrwisiIpOW1ikdp0e31DA3kNP/ba+IiEisOeeeB2yUbb4NfDs+EY2NmVExu2j4kVIzb7R0073QGYSM3PgGKCIicaGR0jHo7XXsqQ/yl/1HeeGtetavKiVUISUiIiIRqJhdxK66Fprbu4beYPFl0N0Ou5+Ja1wiIhI/Gikdg+88vYv/979v9j++skJdd0VERMaiYnYRzsHWqkbOWXhSHyaYezZkFnpdeE99V/wDFBGRmFNSOgZ76oMU52XwxcuXUpKfycJpeX6HJCIiMiGtGtDsaMikNDUdTnkHvPkU9PZCioq8REQmG/3LPgb1wU5mFWWzvmIWZy8Y4gIqIiIiYSnMTmd+SS5/Ga7ZEXjzSoN1UL0pfoGJiEjcKCkdg/rmDorzMv0OQ0REZFLoa3Y0bHPghW8HS4WdT8Uo0eSFAAAgAElEQVQ3MBERiQslpWNQ36KkVEREJFpWlRVR39JBbVP70BtkT4FZlbD7D/ENTERE4kJJaYR6ex1Hgp0E8jL8DkVERGRSWFFWCMC2qsbhN5p/oVe+2z7CNiIiMiEpKY1QY1sX3b1OI6UiIiJRsnRmAakpxrbqUZJS1wN7/xivsEREJE6UlEaoIdgBoJFSERGRKMlKT+WUaXlsHWmktOx0SM/ReqUiIpOQktII1TV3AlCikVIREZGoWVlWyKvVjcM3O0rL9NYsVVIqIjLpKCmNUH2LN1JanK+kVEREJFpWzCqkIdhJTeMwzY7AK+Gt3wFNNfEKS0RE4kBJaYQaQklpIFfluyIiItGyoqwIgG1VI6xXOv9C71ZdeEVEJhUlpRGqb+kkNcWYkqOkVEREJFpOnZFP2mjNjqYtg5yASnhFRCYZJaURqm/pYGpuBikp5ncoIiIik0ZWeiqLpueP3OwoJQXmXeAlpcPNPRURkQlHSWmE6ls6VborIiISAytmjdLsCLwS3pZaqNsRr7BERCTGlJRGqL6lgxI1ORIREYm65WWFHG3tovpY2/Abzb/Qu92jeaUiIpOFktIINQQ7KNZyMCIiIlG3dGYBANsPNg+/0ZS5MGWe5pWKiEwiSkojVN+s8l0REZFYOHVGPmaw/WDTyBvOvxD2PAc93fEIS0REYkxJaQSCHd20dfVojVIREZEYyM1MY+7UnPCS0s5mqNkcj7BERCTGlJRGoKGlE0DluyIiIjGyZGbB6EnpvPMBUwmviMgkoaQ0AnUtHQAE8lS+KyIiEgtLZhaw70grwY4RSnNzpsLMlUpKRUQmCSWlEagPJaUlGikVERGJiSUzC3AO3qgdodkRQPl5ULURutrjE5iIiMSMktII9JXvaqRUREQkNpbMzAfg9dFKeMvPg54OqHo5DlGJiEgspfkdwETSN1IayNVIaUK5/1qvC6OITEwfftIrxRQBZhVlU5CVNvq80jlngqXA3udg3nnxCU5ERGJCSWkEGlo6KMxOJyNNA8wJZc+zUHIqzD3b70hEZCxyAn5HIAnEzDg1nGZH2UUwYyXsfT4+gYmISMwoKY3AviOtTNNyMImlMwhdrbD0Sjj37/2ORkREomDpzAIe2niA3l5HSooNv+G88+DFH0BXG6Rnxy9AERGJKg35helYayd/3FXPhYtL/A5FBgrWe7c5xf7GISIiUbNkZj6tnT3sO9I68obl50FPp+aViohMcEpKw/T4tlq6ehzrK2b5HYoM1BpKSnOVlIqITBZLZhYAhD+vVH0FREQmNCWlYfrllmoWlOSyrLTA71BkoL6R0lyNYIuITBaLpueTYmEkpVmFMHOV5pWKiExwSkrDUH2sjZf2HOHdFbMwG2Fui8Rff/muGqWIiEwWWempLCjJGz0pBZhzNlRvgu7O2AcmIiIxEVZSamaXmtkOM9tlZhuGeH2OmT1tZn8xs61mti76ofrnsS01ACrdTUTBOu9WI6UiIpPKkpkFbD/YPPqGc87w1is9+ErsgxIRkZgYNSk1s1TgO8BlwFLgOjNbOmizLwAPOedWA+8FvhvtQP30+zcOsbKskDmBHL9DkcFa6yEtCzJy/Y5ERESiaMnMAqqPtdHY2jXyhrPP8G4PvBj7oEREJCbCGSldC+xyzu12znUCDwLrB23jgL7JloVATfRC9N+RYCdlU9RqPiEFG7zOuyqrFhGZVJbMzAdge+0oJbz5M6BorpJSEZEJLJykdBZwYMDjqtBzA30ZuN7MqoDHgU9EJboEEezoITdDS7ompGCdOu+KiExCS8PtwAveaOmBF8G5GEclIiKxEE5SOtQQ1OB/9a8DfuycKwPWAf9pZicd28xuNrONZraxrq4u8mh9EuzoJjdTSWlCaq1XUioiMgmV5GcSyM0Is9nRGdByCI7ti31gIiISdeEkpVXA7AGPyzi5PPdvgIcAnHN/ArKAkzIF59wPnXNrnHNrSkomRmMa5xzBzm7ylJQmpr7yXRERmVTMLPxmR/3zSl+KbVAiIhIT4SSlLwOnmNk8M8vAa2T02KBt9gNvBzCzJXhJ6cQZCh1BW1cPvQ6NlCYqle+KiExaS2bms+NQM909vSNvOG0pZOTD/j/HJzAREYmqUZNS51w38HfAU8B2vC67r5nZV8zsytBmnwH+1sxeAR4APuTc5JjYEezoASAvM9XnSOQknUHoblNSKiIySS2ZWUBndy976oMjb5iSCmVr1OxIRGSCCmv4zzn3OF4Do4HPfWnA/deBc6IbWmIIdnQDGilNSFqjVERkUlvS1+yotplTpuePvPHstfDs17wvLLVMmIjIhBJO+W5Sa1FSmriCDd6t5pSKiExK80tySTHYdSiMeaWlleB64eDW2AcmIiJRpaR0FH0jpWp0lIBa671ble+KiExKmWmplAdy2Xm4ZfSNS1d7tzWbYxuUiIhEnZLSUQQ7NVKasPrLd5WUiohMVgun5YWXlOZPh4JZUK2kVERkolFSOooWNTpKXMHQSKnKd0VEJq1Tpuextz5I12gdeMEbLdVIqYjIhKOkdBRqdJTAgnWQlq2GFiIik9gp0/Lp7nXsaxilAy/ArEo4shvajsY+MBERiRolpaNQUprAWhu80l0zvyMREZEYWTgtD4CdhyKZV7olhhGJiEi0KSkdRX/33QwlpQknWA85Ab+jEBGRGFpQkocZanYkIjKJKSkdRbCjm+z0VFJTNBqXcIJ1WqNURGSSy85IpWxKdnhJafYUmDpfzY5ERCYYJaWjaOnoIVdNjhJTX/muiIhMaqdMy2dnOGuVgrdeac1fYhuQiIhElWpSRxHs6E7u+aS9PfCnb8OaD0NmfmT7Bhvg+buguz02sTXXKikVEUkCp0zL4/ld9XT39JKWOsr36aWr4dWHofmQt0yMiIgkvCTOtsIT7OhO7vmkNVvgf78E+aWw8q8j23fn/3gJbVYRpMRgtDl7Csw5O/rHFRGRhLJwWh6d3b0cONrGvOJROq7PqvRua/4Ciy+NfXAiIjJuSZxthaelo5u8ZB4pDdadeDuWfT+9DbIKoheTiIgklVOme5U6Ow81j56UzlwFluI1O1JSKiIyIWhO6SiCnd3JPae0tf7E20j3Tc2IvOxXRERkgP5lYcJpdpSRCyWnqtmRiMgEoqR0FMGOnuSeUxqsP/E2on0bIEfriIqIyPjkZaZRWpjFrnCSUvDmldb8BZyLbWAiIhIVSkpHofLdvvLdsSSldWpEJCIiUbFgWl5kSWlrPTQeiG1QIiISFUpKR9Ga7N13WxtCt2Ms31VSKiIiUXDKtHx2HW6htzeM0c++Zkcq4RURmRCUlI6gt9cR7Ez28t1xNjrKLYluPCIikpROmZ5HW1cP1cfaRt94+nJISfeaHYmISMJTUjqC1q4eAPKSudFR/5zShjHsG5pTKiIiMk6nhJodhVXCm5YJ05d580pFRCThKSkdQbCjGyC5R0r7ync7GqG7M/z9OluhKwi5gdjEJSIiSeV4B97m8HaYVemttd3bG8OoREQkGpSUjqAllJQmbaMj57wS3IzQki6RzCvt21bluyIiEgVFORmU5Gey81C4zY4qoaMJjrwV28BERGTclJSOoH+kNCNJk9LOIHS3w7RTvceRdODt21bluyIiEiWnTMsLb61SgNIK7/bgK7ELSEREokJJ6Qhakr18t6+5UcmpJz4Oa1+NlIqISHSdEloWxoWz/mjJqZCaAQe3xD4wEREZFyWlIwh29DU6StKktG8+aV9S2hpBs6P+8l3NKRURiTYzm21mT5vZdjN7zcw+NcQ2ZmbfNLNdZrbVzCr9iDWaFk7Pp6Wjm9qm9tE3Tk0PNTtSUioikuiUlI7geKOjJO2+2zfaqfJdEZFE0w18xjm3BDgT+LiZLR20zWXAKaGfm4HvxTfE6OvrwBv2vNKZFXBwq9cjQUREEpaS0hGofDdUrjt1AaSkRVi+WwepmZCZH5vYRESSmHPuoHNuc+h+M7AdmDVos/XAT53nz0CRmc2Mc6hRtTCSZWEAZq7yuscf3Ru7oEREZNyUlI4g6ZeE6SvBzZsGOYEIu+82QG4xmMUmNhERAcDMyoHVwIuDXpoFHBjwuIqTE1fM7GYz22hmG+vqIvjy0QeB3Aym5KSHvyzMzFXerZodiYgkNCWlI+hLSnPSk7h8Ny0bMnK9hkURle/WeUmpiIjEjJnlAT8HPu2caxr88hC7nFTH6pz7oXNujXNuTUlJYjenMzMWTsvjrbpgeDtMX+ZV+igpFRFJaEpKR9DS0UNuRiopKUk62hesP949NycQ+ZxSzScVEYkZM0vHS0jvc879YohNqoDZAx6XATXxiC2WFpTk8Va45btpmTBtiTrwiogkOCWlIwh2dCdv6S545bp93XNzSyIs363XSKmISIyYmQH/AWx3zt01zGaPAR8IdeE9E2h0zh2MW5AxsqAkj4ZgJ0eDneHtMHOVN1KqZkciIglLSekIWjq7k3c5GAiV4IZGSnOLIx8p1RqlIiKxcg5wA/A2M9sS+llnZreY2S2hbR4HdgO7gB8BH/Mp1qhaMC0XgN31EXTgbW2ApuoYRiUiIuORxBnX6JJ+pDTYANOWefdziqGjCbo7vHKokXQGoavVK/kVEZGoc849z9BzRgdu44CPxyei+FlY4nV133W4hdPmTh19h5kV3u3BV6CwLIaRiYjIWGmkdAReUpqkTY6cC42U9pXvhkpxwxkt7dtGI6UiIhJls6Zkk5GWElmzI0tRsyMRkQSmpHQEwY6e5C3f7WyBno4Ty3chvHmlfdtoTqmIiERZaooxvzg3/GZHGTlQvBhq1OxIRCRRKSkdQbAzict3+0Y7+zro5kQyUtpw4j4iIiJRtKAkj7fqwkxK4XizIxERSUhJmnGNzjnH0WAnBVnpfodyXEsdHNs39Gv5M47PlXEODm/35nWOpHgRZBV497s74dC2490J697wbvtHSkO3VRshq3Dk41ZvDO2jpFRERKJvwbQ8nnj1IO1dPWSFs5Z4aQVsfRCaa73rpYiIJBQlpcM43NxBU3s3C6fl+R3KcT9dD4dfG/q1jHzYsA9SUr3E8T/eMfrxllwB1/6Xd//5u+CZfz15m8JZ3m3eNG8B8mf+xfsZTUq6t4+IiEiULSjJpdfBvoZWFs/IH32Hmau824NblZSKiCQgJaXDeKO2GSC8i128HNsHp14Op33oxOfffBJe/ndoO+qNTvaNpl7xDSiYNfSxnrkDjg4YdT26zxsNfff3jj+XWQDTlnr3swrgb5+GlkPhxZo/AzJyw9tWREQkAgtKvC+M36prCe86PWOFd3twCyy6OIaRiYjIWCgpHcYbB5sAODVRktKuNq/5UOlqOOWdJ77W3uglpcH6E9cTPfWK491zB3vtEdj9zPHHrfVQUHrysQeauXJcv4KIiEg0zC/xvvTcFW6zo8x8CCzUvFIRkQSlRkfD2FHbzPSCTIpyMvwOxTPSMit9zwXrvNvWeq/9ffaU4Y/Xl7z2zSEN1mkJFxERmRByMtKYVZQdYbOjCiWlIiIJSknpMN6obebUGQV+h3HcSMusDF6uJVgHOQFIGeHjzSn2lnzpaA7t06BuuSIiMmHML8llT32Ya5WCN6+08cDxDvEiIpIwlJQOoaunl12HWxKndBdGXmZl8HItwfrRE8zBiWxrvbrliojIhFEe8JJS11fxM5q+Zke1Gi0VEUk0SkqHsLc+SGdPb2I1OeorzR0qccwJzRsdmJSOlmD2l/zWQ2fQWz5GSamIiEwQ5cW5NLd3cyTYGd4OfUlpzV9iF5SIiIyJktIh9HXenTDlu6lp3vzRSEY9ByayfcmsyndFRGSCmFecA8DehjBLeLOLYOp8JaUiIglISekQ3qhtIjXFWDAtgZY0CdZBaoa3TMtQckuOj6YG60dvWtT3emv9yE2UREREElB5wLtG76lvDX+n0kqoVlIqIpJolJQOYUdtM/OLc8lMS/U7lOP6GhGZDf16TrG3TU8XtB8Lf05psG7kUVgREZEENHtqDqkpxt5Imh2VroamKmg5HLvAREQkYkpKh/BGbTOnzkyg0l0IleQOs+YoeK8F66C14fjjkaRnQ3qul8j2jbDmjLKPiIhIgkhPTaFsSjZ7wi3fBZhV6d2qhFdEJKEoKR3kUFM7VUfbWJpoSelo64jmlkReiptbrPJdERGZsMoDuZGNlM5Y6a3jraRURCShKCkd5Fev1ABw8bLpPkcyyGjLvOQUQ+sRaKk9/ng0ucXHy3fTsiAjgebQioiIjGJesZeUhr0sTGYeFC+G6s2xDUxERCKipHSQR7fUsGJWIQtK8vwO5UStDSPP+cwtBhzU7xrweBQ5xce77440X1VERCQBlQdyCHb2UNfSEf5Opau9kdJwE1kREYk5JaUDvFXXwrbqRtZXlPodyom62qCzJYykFKjbHnocTvluiZfshrOuqYiISIIpL/YqfPZG0oF3ViUED0NTdYyiEhGRSCkpHeDRLTWYwRWrEiwpDWcd0b7XDr8BlgpZRaMft685UrBOSamIiEw48/qT0gg78ILmlYqIJBAlpSHOOR7bUs3ZCwJML8jyO5wT9XXHHa3REXgjpTkBSAnjo80tgZ5OOLpHTY5ERGTCmVWUTVqKRdaBd/pySEnTvFIRkQSipDRkT32QvQ2trFsx0+9QTta/zEsY5bvtjeGPeuYM2EfLwYiIyASTlprCnKk5kY2UpmfBtKUaKRURSSBKSkMOHG0DYNH0fJ8jGUJ/+e4IiWP21OP3w00wByavKt8VEZEJqLw4lz2RJKXgzStVsyMRkYShpDSk5piXlJYWZfscyRDCKd9NTTuemIZbintCUqryXRERmXjKA7nsa2gNf1kY8OaVth/zpq+IiIjvlJSG1BxrIzXFmJ6f6XcoJ2uth9QMyBxlFLcvyYy0fHfwfRERkQliXnEObV09HGqKZFmYSu9W80pFRBKCktKQ6mNtzCjIIi01Ad+SYL03kjnaOqJ9o50aKRURkSTRtyxMRCW805ZAWpbmlYqIJIgEzMD8UX20jdKiBOu62ydYH9480b5twp1Tmp4NGXne/Vw1OhIRkYmnPBBaFiaSDryp6TBjhZJSEZEEoaQ0pKaxLTHnk4JXvhtOSW6k5bswIJFV+a6IiEw8pUXZZKSmRNaBF7wS3pot0NsTm8BERCRsSkqBnl5HbWN74ialwbrwymsjLd/t2zYtGzJyxxabiIiIj1JTjDmBnMg78Jauhq4g1O+MTWAiIhK2NL8DSAT1LR109Thm+ZGU7vsTvPQDeM89kJICLXVw/zXQ0Xx8m8aq8EYy+7aJZNQzt9j7GW2+qoiISIIqD+RGVr4L3rIwADWbYdqp0Q9KRETCpqQUr8kR4E9Suut/4bVH4LKvQt40qH3Fu0DOvwiyp3jbzFwFK68Z/VhLroDgYQgsDP/8Z34MmmvHFruIiEgCmFecw3M76+jtdaSkhPkla2Ch11ehejNUvC+2AYqIyIiUlOI1OQKf1igN1h+/zZsGwQbv8bo7oTiC5BKgYCa87QuR7TP/gsi2FxERSTDlxbl0dPdysKk9/C+YU1K9Et7qjbENTkRERqU5pXhrlAL+dN/tT0rrTryNpFmRiIhIEpvX14E30nmlZadD7TboaotBVCIiEi4lpXhJaUFWGvlZ6fE/eWv9ybcp6ZBVGP9YREREJqAxrVUKXlLa2+114RUREd8oKQWqj/nYebd/pDRUthus85ZpUeMhERGRsMwoyCIzbQzLwpSd7t1WvRz9oEREJGxKSvFGSn1pcgRDlO82RLaki4iISJJLSbGxdeDNK4Ep86DqpdgEJiIiYVFSitd915eR0u5O6Gj07g8s380NxD8WERGRCay8eAxrlYI3WnrgZXAu+kGJiEhYkj4pbenoprGti1lTfEhK+xJROHHEVCOlIiIiESkvzuXAkTZ6eiNMLmevhZZab01wERHxRdInpQePJcByMAPvBxsgR513RUREIlEeyKWzp5eDjRF20i1b492qhFdExDdJn5S+frAJgLlTc+J/8r6R0rzp3v2uduhsVvmuiIhIhPqu4/sbWiPbcfpySMv2SnhFRMQXSZ+U/uqVGmYUZLFilg9LsPSNjpac6pXt9iWpKt8VERGJyNzQsjB7I01KU9NhVqU68IqI+Cipk9KjwU6e2VHHlRWlpKT4sARLX1I6bQm0HYXmQ95jle+KiIhEZEZBFhmpKew7MpZmR2vg4CtexZKIiMRdUielv9l2kO5ex/qKUn8CCNaBpUJgofe4fod3q5FSERGRiKSmGLOnZrOvPsKRUoCytdDbBbVbox+YiIiMKqmT0se21LBwWh5LZxb4E0BrPeQWez8Ah7d7t7kaKRUREYnU3EAu+46MJSk93bs9oGZHIiJ+SNqktOpoKy/tPcK7K0ox86F0F4532u0bGa0LjZTmqNGRiIhIpOYGctjXEMRFuuZo/nQomqMOvCIiPknapPTZN735nOtWzPQviGCd12m3bw5p3XZISYcsH5ouiYiITHBzp+bQ2tlDfUtn5DuXrYWqjdEPSkRERhVWUmpml5rZDjPbZWYbhtnmGjN73cxeM7P7oxtm9FUdbSUtxZgbyPUviNZ6b5S0r1z32H7vvl8jtyIiIhNYXwfefQ1jaXZ0OjRVQ2N1lKMSEZHRjJqUmlkq8B3gMmApcJ2ZLR20zSnA54BznHPLgE/HINaoqjnWxozCLFL96Lrbp698N3sKWOijUOddERGRMelbq3RfpMvCAMwOzStVCa+ISNyFM1K6FtjlnNvtnOsEHgTWD9rmb4HvOOeOAjjnDkc3zOirPtbGrKJs/wLo7oCORm+kNCUVsqd6z6vJkYiIyJiUTckhxcY4Ujp9BaRlqYRXRMQH4SSls4ADAx5XhZ4baBGwyMz+aGZ/NrNLoxVgrNQca/c3KW1t8G5zQ02N+pJRJaUiIiJjkpGWQmlR9tg68KZlwMwKdeAVEfFBOEnpUPWtg9vapQGnABcC1wH/bmZFJx3I7GYz22hmG+vq6iKNNWq6e3qpbWqn1M+kNBj6/fs67w6+FRERkYiVB3LZO5byXYDZa+HgFuhqi25QIiIyonCS0ipg9oDHZUDNENs86pzrcs7tAXbgJakncM790Dm3xjm3pqTEv+TrcHMHPb3O56TU6/7bP4e0bxkYLQcjIiIyZnMCOewfS/kuwNxzoKcTqjdFNygRERlROEnpy8ApZjbPzDKA9wKPDdrml8BFAGZWjFfOuzuagUZTzTHvG9DSoiz/gugv3+0r2y058bGIiIhErDyQw9HWLhrbuiLfec4ZgMG+P0U9LhERGd6oSalzrhv4O+ApYDvwkHPuNTP7ipldGdrsKaDBzF4HngY+65xriFXQ41UdSkrLpiRC+e6guaQq3xURERmzOVO9ZWH2j6WEN3sKTF8G+/4Y5ahERGQkaeFs5Jx7HHh80HNfGnDfAbeGfhJeX1I6szCOSene5+HAi8cf734GUtIgKzT1tr98VyOlIiIiY1Ve7C0Ls7chyIqywsgPMPds+Mt90NMFqelRjk5ERIYSVlI62dQca6MoJ53czDj++r++Fep3nPjcrNPAQn2kSishvxQCC+MXk4iIyCQzJ7RW6f6xdOAFLyl96YdwcCuUnRbFyEREZDhJmpS2UxrPUVKAlkOw5sNw6R3Hn0sZ8A1s2Wnwme3xjUlERGSSyclIY1p+Jnvrx9jsaM7Z3u3+F5SUiojESTiNjiadmmNt8e2829MF7ccgbwakZR7/SUnKt19ERCSm5gZyxrZWKUD+dJi6APa9EN2gRERkWEmZFVUfbYtvk6P+Trta7kVERCTW5gZy2TfWZWHAK+Hd9wL09kYvKBERGVbSJaVN7V00d3THdzmYwWuSioiISMzMnZrDoaYO2jp7xniAs70KpzpNqxERiYekS0qPr1Eax5HS/uVftNyLiIhIrM0tDi0LM55mR6ASXhGROFFSGg/95bsaKRURkfEzs3vM7LCZvTrM6xeaWaOZbQn9fGmo7SaruaEOvGMu4S2aCwWzlJSKiMRJ0nXfrT7qJaVlGikVEZGJ68fAt4GfjrDNc865y+MTTmIpD3gjpfsaxjhSauaNlu55Dpw7vnybiIjERNKNlFYfayc91SjOy4zfSYP1YKmQVRS/c4qIyKTlnHsWOOJ3HImqMCedwux09h0ZR7OjOWdBSy0c2R29wEREZEhJl5TWHGtjZmE2KSlx/NaztR5ypmoJGBERiaezzOwVM3vCzJYNt5GZ3WxmG81sY11dXTzji6nyQM7YR0oB5p7j3e7/U3QCEhGRYSVdluStURrHzrvgjZSqdFdEROJnMzDXObcK+Bbwy+E2dM790Dm3xjm3pqRk8lyr5gRyx5eUliyGnIDmlYqIxEGSJqVxnE8KXlKaozVKRUQkPpxzTc65ltD9x4F0M0uqbnvlgRyqj7XR1TPGtUbNvBLefX+MbmAiInKSpEpKu3p6qW1qj2+TI/AaHWmkVERE4sTMZph53XnMbC3e9b7B36jia87UHHp6XX+DwzGZezYc3QuN1VGLS0RETpZUSemhpnZ6XZyXgwFvTqmWgxERkSgxsweAPwGLzazKzP7GzG4xs1tCm1wNvGpmrwDfBN7rnHN+xeuH8tBapXvHuiwMwLzzvds9f4hCRCIiMpykWhKm5lg7EOektLsT2hshR0mpiIhEh3PuulFe/zbekjFJq2+t0v1HxjGvdNoyr9Lpraeh4n1RikxERAZLqpHSmmNeCU9ck9LWULWURkpFRETipiQ/k+z0VPbWjyMpTUmB+RfC7me89UpFRCQmkiopre5PSuPYfbe13rtVUioiIhI3ZsbcQA77x7NWKcD8iyB4GA69Fp3ARETkJEmXlE7NzSAnI45Vy8HQmm9qdCQiIhJXcwM57B3PsjAACy7ybnc/Pf6ARERkSEmVlPqzRmmofFdzSkVEROJqbiCX/Uda6e0dR+ltQSkUL/bmlWM6FEEAACAASURBVIqISEwkX1Ja6EPnXVD5roiISJzNDeTQ2e0tBzcuCy6CfS9A1ziPIyIiQ0qapNQ5b62yuC8HE6wDS4WsovieV0REJMnNneotC7NvvCW88y+C7jY48OcoRCUiIoMlTVLa1N5NsLOHsinxTkrrISfgdfATERGRuJkb8JaF2TeetUoB5p0HqZnw5lNRiEpERAZLmkyp+qgPy8GAl5SqyZGIiEjclRZlk55q7BvPWqUAGbkw73zY8YSWhhERiYGkSUp9WaMUvDmluYH4nlNERERITTFmT8kZ/0gpwOJL4egeaNg1/mOJiMgJ4rg2ir9qGsexRmlP1/GlXSLVcghKK8e2r4iIiIzLnEDO+OeUApxyCfAZb7S0+JTxH09ERPolTVJafayNjNQUinMzI9/5ZzfAm0+M/eSL3zX2fUVERGTMygO5bNx7FOccZjb2AxXNhunLvXml53wyegGKiEjyJKX7G1qZNSWblJQxXJCO7YOZq2DNh8dwZoNFl45hPxERERmvOVNzaOno5kiwk0DeGL6YHmjRJfD83dB2FLKnRCdAERFJnqR0R20zi6bnjW3nziD/v737jo+juvc+/jm7q14sqxh3yTbNBRnbotuAY+JQgg2hOrRQwoWE8BDCTUzIA7nJzeshhOT6khAnkACBCzYEAvhSE0ooodkGbFwAF2TcrWJbvezuef6YlSzb6lppNKPv+/Xa1+zOzs7+Ritp9NU5cw6jj4dp34prTSIiItK7CnKdEXiLy2riEEpPh7d+DetegcIL4lCdiIjAABnoqK4xQnFZNUcMzezeDhprICE1vkWJiIhIrxsdm6v0y/I4DHY0ogjSD4G1S3q+LxERaTYgQum6nVVELYwfmtG9HTTWKpSKiIh40KjsFIyB4tI4DHYUCMCRX4f1r0BDHPYnIiLAAAmla3dUAHBEd0KptU733USFUhEREa9JCgUZPiiFL3s6V2mTCXOcHlTrX4nP/kREZGCE0s92VJKcECA/J63rLw7XAVYtpSIiIh6Vn5NKcTzmKgXInw4p2bDm2fjsT0REBk4oPWxIBsHujLzb1D1HoVRERMST8nPS4jNXKUAwBEee5UwNE66Pzz5FRAa4ARFKP91RyZHdvp40dhJT910RERFPKshJpby6gYq6xvjscMJcaKiEDa/HZ38iIgOc70NpaVU9pVX13bueFPaFUrWUioiIeFJ+jnMO/zJeraVjToHkQbD6b/HZn4jIAOf7UPrZjkoAjuzudDANsWtQErtxPaqIiIi4rmlMibhdVxpKhAnnwNrn9v2dICIi3eb7UPppLJR2v6W01lkmpMSpIhEREelLTS2lcbuuFKDwQmishk9fiN8+RUQGKN+H0s92VJCbnkheRlL3dtDcfVctpSIiIl6UmhgiLyOJTfFqKQUYfSJkjoRPnojfPkVEBijfh9Li0hrG5qZ3fwfN3Xd1TamIiIhXFeSkUhzPltJAAI46H9a/CtWl8duviMgA5PtQWlHXyKDUhO7voLmlVN13RUREvCo/Jy1+Ax01KbwIbARWacAjEZGe8H0orawLk5Ec6v4O1H1XRETE8/KzU9lRUUdtQyR+Oz1kAgw9Cj58GKyN335FRAYY34fSitpGMpN70FLaoHlKRUREvC4/1/nn8pflcW4tLboadn4Cmz+I735FRAYQX4fSaNRS1RAmMy4tpQqlIiIiXpWf3TQCb5yncCm8EJIyYen98d2viMgA4utQWtUQxlrI6ElLaWMNBJMgEIxfYSIiItKnCmJzlcZ1Whhw5jE/+puw+hmoKonvvkVEBghfh9LKujBAz64pbahR110RERGPG5SaQFZqAsXxbikFOOYaiDbCRw/Hf98iIgOAz0NpIxCHllJ13RUREfG8/OzU+F9TCpB7GIw5BZY9CNE4DqQkIjJA+DyUxqGlVKFURETEF/Jz0nqnpRSc1tK9m+Hzl3tn/yIiPubzUNrUUqruuyIiIgNdQU4qW3fX0hCOxn/nR5wJGcM14JGISDf4PJQ2tZSq+66IiMhANzonjaiFrXtq47/zYAiKroQNr0HZhvjvX0TEx3wdSitiobRHU8I0VCuUioiI+EBBjnM+77UuvFOvgEAIlv65d/YvIuJTvg6l8RnoqFbdd0VERHwgv2lamNJeCqUZh8CEufDRI1C7u3feQ0TEh3wdSitqwyQEDckJPTjMxmpISItfUSIiIuKK3PREUhODbOqNEXibTP8+1FfAe3/ovfcQEfEZX4fSyrpGMpITMMZ0fycNNZCQEr+iRERExBXGGPJz0thU1ouhdOhRcOTX4b2FULun995HRMRHfB5Kwz0beRdi3XfVUioiIuIHBTmpvXdNaZNTfgj1e+H9P/bu+4iI+ITPQ2ljz0KptRp9V0RExEdG56SypbyWSNT23psMm+xMEfPevVC3t/feR0TEJ3weSsNkJPVwkCOsuu+KiIj4REFOGg2RKNv39sK0MC2d8kMnkL5/X+++j4iID/g/lPakpbQxdsJS910RERFfyI9NC9Or15UCDJ8Ch58O7/4O6ip6971ERDzO56G0sYfTwcSuOVH3XREREV9onhamt0MpxFpL98AHai0VEWmPz0NpD1tKG2InLHXfFRER8YVhmckkhgK9P9gRwIhpcOhX1VoqItIB34bSaNRS1RAms0fdd2MnLHXfFRER8YVAwFCQk8rGkqq+ecOZP4ba3fDWr/vm/UREPMi3obSyPoy19LD7buyaUnXfFRER8Y1xeelsLOmDllKAEVOh8GJ47/ewu7hv3lNExGP8G0rrGgHITIlH912FUhEREb8Ym5fGl+U1NEaiffOGs26HQAj+cXvfvJ+IiMf4OJSGgZ62lDZ131UoFRER8YuxuemEo5Yvy/tgsCOAQSPgpJtgzbOw/pW+eU8REQ8ZAKE0DlPCqKVURETEN8YNSQdgw64+uq4U4KQbIe9IeOY7UF3Wd+8rIuIBPg6lTvfdHrWUNmigIxEREb8Zm+ec1zeW9tF1peCM5H/en5xBj5bcANb23XuLiPRzPg6l8Wgp1ZQwIiIifpOZnEBuelLfjcDbZOhRcNpP4bMXYPmDffveIiL9mI9DaVNLqQY6EhERkf2NzUtjQ1+NwNvScdfDuK/ASz+Gks/7/v1FRPoh34bSilhLaWaPBjqqgVAyBIJxqkpERET6A2damD5uKQUIBOCchU4vrKeuhnBD39cgItLP+DaUVtaFSQgakkI9OMTGGnXdFRER8aFxeWnsrmlkd7ULoTBjKMy9F3ashL/f1vfvLyLSz/g4lDaSkZyAMab7O2mogQQNciQiIuI3+wY7cqG1FODIM+GEG+CD++DDh92pQUSkn/BtKK2oC/fselJwWko1R6mIiIjvjMtrmhbGhetKm5z2HzB2Jjx3M2z+wL06RERc5ttQWlnX2LPrSUHdd0VERHxq5OBUEoMBNrjVUgoQDMH5D0DmcHjyKqjd414tIiIu8nEojUNLqbrvioiI+FIwYMjPSWWjGyPwtpSa7QTTim3w/M2av1REBiQfh9JGdd8VERGRNjnTwrjYUtpkZBGceiusegpWLHK7GhGRPufbUFpVFyY9Sd13RUREpHXj8tL5sqyGxkjU7VJgxs2QPx2e+z5s+8jtakRE+pR/Q2l9mPSkHs4vqu67IiLSDxljHjDG7DLGrGrjeWOMuccYs94Ys9IYM7Wva/SCsXnphKOWzeU1bpfizIl+wUOQmguLL4GqXW5XJCLSZ3wZSq21VDdESEvq6TWllZCoUCoiIv3OQ8Dp7Tx/BnBY7HYtsLAPavKc5mlh3L6utEl6HsxbBDXlsPib0NBP6hIR6WW+DKX14SiRqO1ZKI2EoXY3pOXGrzAREZE4sNa+CZS3s8lc4GHreA/IMsYM65vqvGNcbmxamP5wXWmTYYVw3p9g63J4/DIIN7hdkYhIr+tUKDXGnG6M+SzWDWh+O9udb4yxxpii+JXYdVX1YQDSexJKa2Pn+lSFUhER8ZwRwOYWj7fE1kkLg1ITyE1P7D8tpU3Gfx3Ovgc2vAp/+zZEI25XJCLSqzoMpcaYIHAvTlegCcA8Y8yEVrbLAG4E3o93kV1VHQulPWoprS51lmopFRER7zGtrGt1rhFjzLXGmGXGmGUlJSW9XFb/MzY3nY1uzlXalqmXwexfwJpnnMGPNFWMiPhYZ1pKjwXWW2s3WmsbgMU43YIO9HPgLqAujvV1y76W0h4MdFQdOzErlIqIiPdsAUa1eDwS2Nbahtba+6y1Rdbaory8vD4prj9xpoXpZy2lTU68AWbcAh/+Bf5xu4KpiPhWZ0Jph12AjDFTgFHW2ufa21Ff/Te2ut7p5tKjltKappbSgXeCFhERz1sCXB4bhfd4YK+1drvbRfVH4/LSKa9uYE9NP7128ys/gWOugXfuged/oK68IuJLnUlt7XYBMsYEgP8CvtXRjqy19wH3ARQVFfXav/vi0323zFnqmlIREelnjDGLgFOBXGPMFuAOIAHAWvsH4AXgTGA9UANc6U6l/V/TCLwbSqqZlp/ocjWtMAbO+BUkpsO/FkDldjjvz5CY6nZlIiJx05nU1lEXoAxgEvBPYwzAUGCJMWaOtXZZvArtirgMdFRdAhhIzY5PUSIiInFirZ3XwfMW+G4fleNpY/OcEXg3llQxLX+wy9W0IRCAr/4HZI6AF38ID8+BeY9DWo7blYmIxEVnuu8uBQ4zxowxxiQCF+N0CwLAWrvXWptrrS2w1hYA7wGuBVKIU0tpTakTSAM9uC5VRERE+rVRg1NICJr+e11pS8ddCxc+DDs+gT9/Fco2uF2RiEhcdBhKrbVh4AbgZWAt8IS1drUx5mfGmDm9XWB3NLeUJvawpVRdd0VERHwtFAyQn5PGxv40V2l7JsyBy5915lK//yuw8Z9uVyQi0mOdmqfUWvuCtfZwa+04a+0vYutut9YuaWXbU91sJYWWAx31ZPTdMg1yJCIiMgCMy0tjvVdCKcDo4+Hbr0HGMHjkG/D2Ag2AJCKe1qlQ6jXVDWGSQgFCwR4cXk2prtUQEREZAA4/JINNZTXUhz0U7LLHwDX/gPFfh1fugIfnwt4tblclItItvgylVfXhng1yBOq+KyIiMkAcOiSdSNTyRakHrittKSkDLvgLzL0Xtn4IC0+EVX9zuyoRkS7zZSitrg/3bJCjSNi5VkPdd0VERHzv8EMyAPh8p4e68DYxBqZcCte9BTmHwZNXwuJLYOdqtysTEek0hdLW1JY7yzS1lIqIiPjd2Lw0AgbW7ax0u5TuyxkHV70EM38CG99wWk2fuFzhVEQ8wZeh1Om+25NBjkqcpUKpiIiI7yWFghTkpLHOiy2lLQUT4JR/h5tWwsn/DutfUzgVEU/wZSitro+Q2qPpYEqdpa4pFRERGRAOOySdz3d5uKW0pdRs+MpPDg6nf70Sqkrcrk5E5CA+DaU9HOioJhZK1VIqIiIyIHhyBN6OHBhOP33eCafrX3G7MhGR/fgylFbVh3s4R2lTKNVARyIiIgPBYYdkeHME3s5oCqfffs25/z/nwcu3Qbje7cpERACfhtKahkjPBjqqLgUMpAyOW00iIiLSfx02JB3w6Ai8nTV0Elz7TzjmGnj3d/CnWbBjldtViYj4L5Raa6lu6GH33eoS5z+JgR60toqIiIhnjM1LIxgw3h6BtzMSUuCsX8PFi6BiO9x3Crz2n2o1FRFX+S6U1jREsJaetZTWlKrrroiIyACSFAqSn5Pq/RF4O+vIM+G7H8BRF8Cbv4I/TIcv33O7KhEZoHwXSqvrw0APQ2l1mUbeFRERGWAOG+KjEXg7Iy0Hzv0DXPoUNNbBA1+D538AdRVuVyYiA4zvQmlVLJT2eJ5SjbwrIiIyoBwxNJPi0mrqGn00Am9nHHoafOddOP47sPTP8Pvj4ZMnIRJ2uzIRGSB8F0qr650TSVpP5imtKVUoFRERGWAmDMsgauGzHQOotbRJUjqc/v/gmlcgOQueuhp+OxU+uB8aatyuTkR8znehdF9LaRdDaU053DUWfjoIandD2pBeqE5ERET6qwnDBgGwZvsA7r46sgiuexsuehTSh8ALt8CCSc5gSKXr3a5ORHyqB82J/VO3rykt3wg1ZXDUhZBzKEy9rBeqExERkf5q5OAU0pNCrB3IoRQgEIDxX4cjz4Iv34V//Te8ebczINKIaXDstTDxXAgluV2piPiE/0JpQzdDaXWpszz+OucXroiIiAwogYDhyKEZCqVNjIH8E51bxXZY9RR8+DA8/W/wj9vhK/8Xjr7ECbEiIj3gu98i3e6+W13iLDXqroiIyIA1flgmn26vxFrrdin9S+YwOPEG+O77cNnTkJUPS26A+2fC538Hfb1EpAd8F0r3dd/t4ui7NbGWUg1wJCIiMmCNH5ZJZX2YLbtr3S6lfzIGxn0Frv47fONPzqVPj10A950Ka5+DaNTtCkXEg3wXSqu6O/pudSkkpEJiWi9UJSIiIl4wflgGMMAHO+oMY6DwAvjehzDnd1C3Fx6/BP5wkhNO1XIqIl3gu1BaXR8mNTFIIGC6+MJSdd0VEREZ4I4YmoExsGabQmmnhBKdwSFvWAbfuB8ijU44fegs2Lrc7epExCN8GUq7PMgRxOYmzYl/QSIiIuIZqYkhxuSmabCjrgqGoPBC+M57cNZvoOQzuP8r8NQ1zgwHIiLt8F0oraoPd32QI3AGOkrLi39BIiIi4injh2WydodCabcEQ3DM1XDjRzDjFlj7v3DPVPif8+DT5yESdrtCEemHfBdKnZbSLg5yBFBdpu67IiIiwoRhmWwur6WyrtHtUrwrORNm/V+48WM45UewczUs/iYsOApe/TlsXgrRiNtVikg/4cNQGiG1q4McWRtrKVUoFRERGeiaBjv6dEely5X4QOYwmHkr3LQKLnoUhhwJb/8G/nwa/OpQePJqWLEYqna5XamIuKgb/Vz7t6r6MEMHJXftRQ1VEKlXKBURERHGD8sEYO32Co4pyHa5Gp8IhmD8151bTTlseA3Wv+LcVj3pbDP0KBg3y5lyJv9ECCa4W7OI9BnfhdLqhm4MdFQdm6NU3XdFREQGvKGZyWSlJmgE3t6Smg1Hne/colHYscIJqRteh3fvhX8tgOQsOPLrcPhsKJjhvEZEfMt/obQ+THpXryltCqUa6EhERGTAM8YwfmimRuDtC4EADJ/i3Gb8AOor4Ys3Yc0SWLsEPv4fwMDQSVBwMow52WlFTc50u3IRiSPfhdKq+jBpXb2mtKYplGpKGBEREXG68D72wSYiUUuwq3OfS/clZcCRZzm3cANs+xC+eAu+eAOW/gneuxdMEIYf7QTUghkw+nhITHO7chHpAV+F0vpwhLrGKBnJXbwGobrEWaqlVERERIAJwzOpa4zyRWk1hw5Jd7ucgSmU6ATO0cfDKf8OjXWw5QOnJfWLt+Cd38Lb/wWBBBh5DIyZ4QTVkcdAKMnt6kWkC3wVSsurGwDIzUjs2gt1TamIiIi00DQC79rtFQql/UVCshM6x5zsPK6vgs3vxULqm/Dmr+CNX0IoGUYdByOLYNhkGHY0ZI0GoxZvkf7KV6G0tDIWStO7+N+xmjJISIXE1F6oSkRERLzm0CHphAKGtdsrOHvycLfLkdYkpcOhpzk3gNo9sOkdJ6AWvw1vLwAbmws1OQtGHQsF050uv8MmQ6Ab89qLSK/wVyitqge6EUo1R6mIiIi0kBQKcuiQdA125CUpWXDkmc4NnO6+u1bD9hWw7SPY9C6s+7vzXNIgpyV1yHgYXOBckxpKdhopElL23ZKznJF/E1LV0irSi3waSrvRfVddd0VERKSF8cMyeWdDqdtlSHclJMOIac6tScV22PQvpzV124ew9F8Qrut4X8EkSBnsBNVAyAmxqdmQMQyyx8CgUZCY7rTeJqY7z7dchrr4t6nIAOOzUNrN7rvVJc4vFREREZGY8cMyePqjrZRV1ZPT1b8tpH/KHLZvjlRw5kmtKYXGGmis3XcL10JDtdMluHY31JZDTbkTYCONsefKYdda+Hh7x+8bTISkTEge5LToJg9yWl8DQSfkBkIttsl0lkkZzv3E9FhXYwMm4LTYBhOdwZxSBkP6IRrYSTzPZ6G0npSEIGlJXZ0SpgyGHtU7RYmIiIgnTRo+CIBV2yo45XCN0O9LgQCkD+nZPhqqoXIHNFQ59xuqnflWm+43VDqDMtVXQN1eJ+jW7YGqXRAN77s11jmva6zueg2pOU4DS2IaNNRAtDHWHTnWDTmU4rQcN99PccJxWp7TfTlvPKTre1zc46tQ6vwns4vdI6yNdd/VHKUiIiKyz6SRgzAGVmzeo1AqbUtMg5xx8dtfJBwLspVQV+GE3WgEsM7frTbqhM7GOqe1tnLHvltDJaRkQzAE4Xqn1beuAsK79m8Fbqw9uNtyaq5zjW3u4ZA1ygm5TQOBJqQ5odYE97XWmoBza9mC23L9fs8d8BpjnKl8ggnOMhCI39dPPMlXobS0qqHrXXfrKyFSrzlKRUREZD+ZyQmMzU1j5ZY9bpciA0kw5HTLTRncu+/TUONcwla+wemG3HRb/Teny3JfCoRiITXRCarNt8RYaA0d0NU5Yf/HTc8HE1vs48D7Cc61wW0+3879UOLB6xWm48pnobSekYO7OK1LTWwAA42+KyIiIgeYPDKLN9eVYq3FaPRV8ZPEVEjMh8H5MO4r+z9XX+l0L26sccJrY7XTMmuj+27EWm3tgcuWzx24nlhLb8TpshxpcK7RjTbuux+J3W9+vsG59jcadraLhp3XNzbs6/ocafFc0+sjDRBuen1j73wNm64FbmrxNd0Iqf3198rlS2DIkX32dj4LpQ0cPSqr/Y12roZ3f7/vB6O23Flq9F0RERE5wORRWfzto61s31vH8KwUt8sR6RtJGc7NL6zdP6y2ev/AdfXtbNvGfWu7WlivHG5c9PHn75tQGolayqvrO+6+u2IRfPyoM3R3kyETNNCRiIiIHKRwpDPY0cotexRKRbzKGKcLrqbm6bd8E0p31zQQtZ2Yo7S6FAaNhO9/0jeFiYiIiGeNH5ZJQtCwYsteTp+k6eNERHqDb67OLYvNUdrhPGIaaVdEREQ6KTkhyJFDM1mxWYMdiYj0Ft+E0tKqeoCOu+/WlGpQIxEREem0wpGD+GTLXqLRfnz9l4iIh/kulOZldKL7rqZ/ERERkU6aPCqLyvowG0ur3S5FRMSXfBRKne67HbaUqvuuiIiIdMGU2Mj+H33Zx3M3iogMED4KpfWEAobM5IS2N2qohnCtWkpFRESk08blpZOVmsCyYoVSEZHe4J9QWllPTnoigUA7E9BWlzhLXVMqIiIinRQIGIryB7N0U7nbpYiI+JJvQmlZdUMnuu6WOctUhVIRERHpvKKCbDaWVFMWG8NCRETixzehtLSqvhPTwTS1lKr7roiIiHTeMQWDAVi2SV14RUTizT+htLKe3PQORt6tKXWWaRroSERERDpv0ohBJIYCLCtWF14RkXjzRSi11lJa3UBeZ0beBbWUioiISJckhYIcPTKLpRrsSEQk7nwRSivrwzSEo+R01FJaXQKhFEhM65vCRERExDeKCgazauteahsibpciIuIrvgilFbWNAAxKaWc6GICaMo28KyIiIt1yTEE24ajl48173C5FRMRXfBFK68NRwOla067qEoVSERER6ZapowdjDLquVEQkznwRShtioTQx1MHhVJdqOhgRERHplkGpCRxxSAZLNQKviEhc+SuUBjs4nJoyDXIkIiIi3TYtfzAfbdpNJGrdLkVExDf8EUojnWgptTbWfVfTwYiIiEj3HFOQTWV9mM92VLpdioiIb/gilDZ2pvtuQzWE69R9V0REfMEYc7ox5jNjzHpjzPxWnv+WMabEGPNx7HaNG3X6TVHBYACWbdJ1pSIi8eKLUFrfmZbS6hJnqe67IiLiccaYIHAvcAYwAZhnjJnQyqaPW2uPjt3+1KdF+tSIrBSGDUrWfKUiInHki1DaqWtKa8qcpUbfFRER7zsWWG+t3WitbQAWA3NdrmlAMMZQVJDN0i/KsVbXlYqIxIOvQmlSuy2lpc5SoVRERLxvBLC5xeMtsXUHOs8Ys9IY86QxZlTflOZ/xxQMZkdFHVv31LpdioiIL/gqlHaq+66uKRUREe8zraw7sNnuf4ECa20h8Arwl1Z3ZMy1xphlxphlJSUlcS7Tn4ryswFYpi68IiJx4Y9Q2plrSmvUUioiIr6xBWjZ8jkS2NZyA2ttmbW2PvbwfmBaazuy1t5nrS2y1hbl5Wnchc44YmgGmckh3t1Q5nYpIiK+4I9Q2plrSqtLISEVEtP6qCoREZFesxQ4zBgzxhiTCFwMLGm5gTFmWIuHc4C1fVifrwUDhhPG5fD2+lJdVyoiEgf+CqXttpSWQ6rmKBUREe+z1oaBG4CXccLmE9ba1caYnxlj5sQ2u9EYs9oYswK4EfiWO9X60/TD8ti6p5ZNZTVulyIi4nkhtwuIh051322oUiupiIj4hrX2BeCFA9bd3uL+rcCtfV3XQDH9UOdyoLfXl1KQq78vRER6whctpfWd6b7bWON03xURERHpoYKcVEZkpfD2ulK3SxER8TxfhNKGcJTEYABjWhuMsGmjGrWUioiISFwYYzjp0Bze2VBKJKrrSkVEesI/obS9rrugllIRERGJq5MOzaWiLsyqrXvdLkVExNP8EUojkU6G0pS+KUhERER876QW15WKiEj3+SOUxrrvtr+Ruu+KiIhI/OSmJzFhWCZvfFbidikiIp7mn1Cq7rsiIiLSx2aNH8KyTeXsqWlwuxQREc/yRyiNdDaUqvuuiIiIxM9XjhxC1MI/1VoqItJt/gil4SgJ7XXfjUYgXKfuuyIiIhJXk0dmkZueyKuf7nK7FBERz/JFKK3vqPtuY42zVPddERERiaNAwDDziCG88dkuGiNRt8sREfEkX4TShnCUpPZaShtrnaW6vrYwCgAAIABJREFU74qIiEiczRo/hIq6MMuKd7tdioiIJ3UqlBpjTjfGfGaMWW+Mmd/K8zcbY9YYY1YaY141xuTHv9S2dXhNaUO1s1T3XREREYmz6YflkRgM8NqnO90uRUTEkzoMpcaYIHAvcAYwAZhnjJlwwGYfAUXW2kLgSeCueBfang5H31X3XREREekl6Ukhjh+Xw8urd2KtdbscERHP6UxL6bHAemvtRmttA7AYmNtyA2vt69baWPLjPWBkfMtsX4fzlDZ131VLqYiIiPSCs44aypflNazaWuF2KSIintOZUDoC2Nzi8ZbYurZcDbzYk6K6qtPdd3VNqYiIiPSCr00cSihgeG7lNrdLERHxnM6EUtPKulb7phhjLgWKgF+18fy1xphlxphlJSXxm89L3XdFRETETVmpiUw/LJfnVm5XF14RkS7qTCjdAoxq8XgkcNC/AY0xpwG3AXOstfWt7chae5+1tshaW5SXl9edelvVYSjVQEciIiLSy846ahhb99SyYstet0sREfGUzoTSpcBhxpgxxphE4GJgScsNjDFTgD/iBNI+nz2609eUqvuuiIiI9JLZE4eSEDQ8t0JdeEVEuqLDUGqtDQM3AC8Da4EnrLWrjTE/M8bMiW32KyAd+Ksx5mNjzJI2dtcr6iNRkjrVfVctpSIiItI7BqUkcPJhefzvym2EI1G3yxER8YxQZzay1r4AvHDAuttb3D8tznV1mrW2C913dU2piIiI9J4Likby6v/s4o3PS5g1/hC3yxER8YTOdN/t1xojzmACHXffNRBK7puiREREZECaNf4QctOTWPTB5o43FhERwAehtCHWPabD0XcTUsG0NpCwiIiISHwkBAOcP20kr3+2ix1769wuR0TEE7wfSsOdCKUN1eq6KyIiIn3i4mNGEYla/rpMraUiIp0xMEJpY63mKBUREZE+UZCbxonjcli8dDORqOYsFRHpiOdDaWNT9912rymtVigVERGRPnPZ8fls3VPL31fvcLsUEZF+z/OhtL5T3Xdr1H1XRERE+szsiUMZnZ3KH9/ciLVqLRURaY/nQ2lT990O5ylVS6mIiIj0kWDAcM2MMXy8eQ/LN+12uxwRkX7N+6G0K6PvioiIiPSR86eNJCs1gfve3Oh2KSIi/Zr3Q2lT991gsJ2N1H1XRERE+lZqYojLjs/nH2t3sm5npdvliIj0W/4JpR22lKb1UUUiIiIijitPGkNqQpAFr65zuxQRkX7L+6E0EgE6EUrVUioiIiJ9LDstkStPGsPzK7fz6Y4Kt8sREemXvB9Kw52YEqahBhJS+qgiERERkX2umTGGjKQQ//WPz90uRUSkX/J8KO1wSphoBCL16r4rIiIirshKTeTqGWN4efVOPt68x+1yRET6Hc+H0g6nhGmodpbqvisiIiIuuXr6GIZkJHH7s6uIRDVvqYhIS94PpR1NCdNY6yzVfVdERERckpGcwG1njWfllr08vnSz2+WIiPQr3g+lHV1T2hhrKVX3XREREXHRnMnDOW5MNne9/Cnl1Q1ulyMi0m/4J5S22X23xlmq+66IiIi4yBjDz+ZOoqouzK1/W4m16sYrIgIDIZQ2d99VKBURERF3HTE0gx+efgQvr97Jo+9/6XY5IiL9gvdDaSSKMRAKmNY3aO6+q1AqIiIi7rtm+lhOPjyPnz+3RnOXiojgh1AajpIYDGBMG6FU3XdFRESkHwkEDL+5cDKDUhL4t0eWs7em0e2SRERc5flQWh+Ott11F6AxFko10JGIiIj0E7npSSy8dCrb9tTyfx7/SNPEiMiA5vlQ2hCJtj1HKbQIpZoSRkRERPqPafnZ3HH2RP75WQn/+fwaDXwkIgNWyO0Ceqqp+27bGzR131VLqYiIiPQvlxw3mg0lVTz4r2KshTvOntD2JUkiIj7lj1DabkupBjoSERGR/skYw+1fn0DQGP709hc0RqL8fO4kAm0N4Cgi4kO+CKUJ7bWUNtYCBkJJfVaTiIiISGcZY7jtrPEkhAIs/OcGGiNR/t83CgkqmIrIAOH9UBrpoKW0ocbpuquuMCIiItJPGWP44deOICEY4J5X11HbGOWu8wpJSQy6XZqISK/zfijtTPdddd0VERGRfs4Yw81fPZyUhCB3vfwp63ZW8sfLppGfo3ExRMTf/BFKOxroSHOUinhSY2MjW7Zsoa6uzu1SpB9JTk5m5MiRJCQkuF2KSK+4/tRxjB+WwU2Pf8xZ97zN7V+fwAVFIzUAkoj4ludDaX0kSmZyO4fRWKOWUhGP2rJlCxkZGRQUFOiPMQHAWktZWRlbtmxhzJgxbpcj0mtOPWIIz31vOrf8dQU/fGolL67azm1nTeDQIelulyYiEnfeDqV7vuTiyr+Q0RCEV//e+jY7V0NqTt/WJSJxUVdXp0Aq+zHGkJOTQ0lJiduliPS6kYNTeeya43nonWJ+/ffPmP1fb3DBtFHc9NXDGDZI86+LiH94O5RWbOPCur9CHfB2O3+0Hnpan5UkIvGlQCoH0veEDCSBgOGq6WOYe/Rw7n19A//z3iae/ngr3zqxgCtPKlA4FRFf8HYoHX08p6Y+xbTRg1lw8RS3qxERHykrK2PWrFkA7Nixg2AwSF5eHgAffPABiYmJHe7jyiuvZP78+RxxxBFtbnPvvfeSlZXFJZdcEpe6d+7cyYgRI/jjH//I1VdfHZd9ioj7ctKTuP3sCVx5UgELXlnH/W9t5E9vbeTUI4Yw79jRzDwij1B7Y2yIiPRj3g6ldGL0XRGRbsjJyeHjjz8G4Kc//Snp6enccsst+21jrcVaSyDQ+u+gBx98sMP3+e53v9vzYlt4/PHHOeGEE1i0aFGvhtJwOEwo5PlTiIjnjMpO5dcXTub/zDqMx5d9yV+XbeHbDy/jkMwk5kwezumThjFlVBYBzXEqIh7i+TSnUCoifWn9+vVMmjSJ6667jqlTp7J9+3auvfZaioqKmDhxIj/72c+at50+fToff/wx4XCYrKws5s+fz+TJkznhhBPYtWsXAD/5yU9YsGBB8/bz58/n2GOP5YgjjuCdd94BoLq6mvPOO4/Jkyczb948ioqKmgPzgRYtWsSCBQvYuHEjO3bsaF7//PPPM3XqVCZPnszs2bMBqKys5IorruCoo46isLCQZ555prnWJosXL+aaa64B4NJLL+UHP/gBM2fO5Mc//jHvvfceJ5xwAlOmTOGkk05i3bp1gBNYv//97zNp0iQKCwv5/e9/z8svv8wFF1zQvN8XX3yRCy+8sMefh8hANTonlX//2pG8M/8r3HfZNCYNH8RD7xRz3sJ3OPHO17jj2VW8+XkJVfVht0sVEemQ5//N7UwJo4mlRfzuP/53NWu2VcR1nxOGZ3LH2RO7/Lo1a9bw4IMP8oc//AGAO++8k+zsbMLhMDNnzuT8889nwoQJ+71m7969nHLKKdx5553cfPPNPPDAA8yfP/+gfVtr+eCDD1iyZAk/+9nPeOmll/jtb3/L0KFDeeqpp1ixYgVTp05tta7i4mJ2797NtGnTOP/883niiSe48cYb2bFjB9dffz1vvfUW+fn5lJeXA04LcF5eHp988gnWWvbs2dPhsW/YsIFXX32VQCDA3r17efvttwkGg7z00kv85Cc/4fHHH2fhwoVs27aNFStWEAwGKS8vJysrixtvvJGysjJycnJ48MEHufLKK7v6pReRA4SCAWZPHMrsiUOpqGvktbW7eHHVdh5ftpm/vLuJYMAwcXgmxxRkc+yYbI4pyCY7rePLD0RE+pLnQ2ljxKqlVET61Lhx4zjmmGOaHy9atIg///nPhMNhtm3bxpo1aw4KpSkpKZxxxhkATJs2jbfeeqvVfX/jG99o3qa4uBiAt99+mx/96EcATJ48mYkTWw/SixYt4qKLLgLg4osv5rvf/S433ngj7777LjNnziQ/Px+A7OxsAF555RWeeeYZwBk8aPDgwYTD7beqXHDBBc3dlffs2cPll1/Ohg0b9tvmlVde4aabbiIY+4dh0/t985vf5LHHHuOSSy5h+fLlLFq0qN33EpGuyUxO4JwpIzhnyghqGsIs37SbpV+U8/4X5Tzy3ib+/PYXABw6JJ2jR2U1344YmkGCrkcVERd5OpRaa2mIqPuuyEDQnRbN3pKWltZ8f926dfz3f/83H3zwAVlZWVx66aXU1dUd9JqWAyMFg8E2w19SUtJB21hrO1XXokWLKCsr4y9/+QsA27Zt44svvsBa2+qIta2tDwQC+73fgcfS8thvu+02vva1r/Gd73yH9evXc/rpp7e5X4CrrrqK8847D4CLLrqoObSKSPylJoaYcVgeMw5zBmirD0f4ZMte3v+inGXF5bz26S6eXL4FgKRQgDG5aYzLS2dsXhpjctMYm5fOmNw0BqUkuHkYIjJAeDqUNkSigPPLVETEDRUVFWRkZJCZmcn27dt5+eWXm8NZvEyfPp0nnniCGTNm8Mknn7BmzZqDtlmzZg2RSIStW7c2r7vttttYvHgxV111FTfddBObNm1q7r6bnZ3N7Nmz+d3vfsfdd9/d3H138ODBDB48mHXr1jFu3Diefvrp5lGHD7R3715GjBgBwEMPPdS8fvbs2SxcuJAZM2Y0d9/Nzs5m1KhR5Obmcuedd/L666/H9WskIu1LCgUpKsimqMDpuWCtZcvuWj7evIeVW/awoaSa1dv28tLqHUSi+/4xlZueyNjcdPJzUhmVncrIwSmMHOwsD8lMJqgBlUQkDrwdSsNOKE1UlxMRccnUqVOZMGECkyZNYuzYsZx00klxf4/vfe97XH755RQWFjJ16lQmTZrEoEGD9tvmscce49xzz91v3XnnnccVV1zBrbfeysKFC5k7dy7WWoYPH86LL77IHXfcwXe+8x0mTZpEMBjk5z//OXPmzOGXv/wlp59+OqNHj2bChAnU19e3WtePfvQjrrrqKu666y5mzpzZvP7f/u3fWLduHYWFhYRCIa6//nquu+46wOnCW1FRweGHHx7nr5KIdIUxhlHZTtA8e/Lw5vUN4ShfltfwRWk1G0uqYstq3vi8hF2V+/8uCAUMw7NSYkE1hVGDUxmZncLQzBSGDkpmaGYyKYnqESEiHTOd7RYWb0VFRXbZsmU92kdZVT3T/vMV/mPORK44sSA+hYlIv7F27VrGjx/vdhmuC4fDhMNhkpOTWbduHbNnz2bdunWenJLluuuu44QTTuCKK67o0X5a+94wxiy31hb1aMcDXDzOzeJfdY0Rtu+tY3N5DVt217Jl9/7LA0MrQGZyiEMykxk6KJkhGckMHZTEIZnJ5KUnkZeRxJCMZPIykhReRXyqs+dm7/1F00JT911dUyoiflZVVcWsWbMIh8NYa/njH//oyUB69NFHM3jwYO655x63SxGRbkhOCDIm17nmtDV1jRG27qll5946dlQ4t51769hZUc+OijrW7yplV2X9ft2Dm6QnhcjLSCI7LZHstERympbpSeSmJ5KTtu+5QSkJJCcEWr12XUS8yXt/1bSg7rsiMhBkZWWxfPlyt8vosbbmVhURf0hOCDIuL51xeeltbhOJWsqrGyiprKekqt5ZVtazq7KOksp6yqsb2Fxew0df7mF3TUOrARacv/0yUxIYlBJiUErCQbfMlAQykkOkJTm39KQQaYmxZVKQtKQQSSEFW5H+wh+hVC2lIiIywBhjTgf+GwgCf7LW3nnA80nAw8A0oAy4yFpb3Nd1irQUDBjyMpyuux2JRi0VdY2UVjVQVlVPWXUDu2sa2FvbyN7aRipiy721zjYbSqqd9XWNdObqtFDA7AussaCa3vz44PVpifuvb9ouLSlEWmKQYMAo5Ip0k6dDab1CqYiIDEDGmCBwL/BVYAuw1BizxFrbcmjmq4Hd1tpDjTEXA78ELur7akW6JxAwZKUmkpWayKFD2m59PVA0aqmsD1NVH6a6xdK5H9lvXdV+z0eorAuzY2/dvm0aIm221h7IGKcFNzEYICHUtDQkxNYlhgLN953nTfO6hNjzicEACQesTzpgm4Sgad5fKBggaAzBgCEUNASMIRRwHgcD+993HgcIBCAUCBy8jTEENJqyuMTToVTXlIqIyAB1LLDeWrsRwBizGJgLtAylc4Gfxu4/CfzOGGOsWyMcivSRQMA0d+PtKWst9eGoE17rWgTYhn0Bt7o+TE1DhMZIlIZIlIZwlMZIlMawdR5HojSGY8vY+praRhpj2+173tIQjtAYsTRGooQ7GYbjyRiccGoMxjiPAy0f43x9nXUAzrLpsYlt1+ZjnMft17D/BgdufuDrD3p8wCsOfr7tHXb4Xgc938VaD9yiw/23//qefi0O1PJ4fnHOJEZlp7b/gjjydChNDgU5elQWg1MTO95YRETEP0YAm1s83gIc19Y21tqwMWYvkAOU9kmFIj5gjCE5IUhyQpDc9I67HMdTJOqE08bmoOs8rg/vWxeOWiItbuFolKi1hCOxdTa2PtLiftQSjS0j0SiRKESi++/LAlFrsdYJ5lHb+mMnN1ui0X2PLc52zY9ty8f7B+0D/0V2YAw/+F9oXX297eD5zr/2oEoOeu8Oauvq9geus/tX2ZnX7/98578WQJ//U8TToXTC8Eye+W785wQUEQE49dRTufXWW/na177WvG7BggV8/vnn/P73v2/zdenp6VRVVbFt2zZuvPFGnnzyyVb3fffdd1NU1PYo6QsWLODaa68lNdX5T+WZZ57JY489RlZWVg+Oap/JkyczYcIEFi1aFJf9SZ9q7f/dB/4F0ZltMMZcC1wLMHr06J5XJiJx4XSvdQKxiN+p36uISBvmzZvH4sWL91u3ePFi5s2b16nXDx8+vNVA2lkLFiygpqam+fELL7wQt0C6du1aotEob775JtXV1XHZZ2vC4XCv7XuA2wKMavF4JLCtrW2MMSFgEFB+4I6stfdZa4ustUV5eXm9VK6IiEjbFEpFRNpw/vnn89xzz1Ff70wIX1xczLZt25g+fXrz3KFTp07lqKOO4tlnnz3o9cXFxUyaNAmA2tpaLr74YgoLC7nooouora1t3u7666+nqKiIiRMncscddwBwzz33sG3bNmbOnMnMmTMBKCgooLTU6Xn5m9/8hkmTJjFp0iQWLFjQ/H7jx4/n29/+NhMnTmT27Nn7vU9Ljz32GJdddhmzZ89myZIlzevXr1/PaaedxuTJk5k6dSobNmwA4K677uKoo45i8uTJzJ8/H3Bae5ctWwZAaWkpBQUFADz00ENccMEFnH322cyePbvdr9XDDz9MYWEhkydP5rLLLqOyspIxY8bQ2NgIQEVFBQUFBc2PpdlS4DBjzBhjTCJwMbDkgG2WAFfE7p8PvKbrSUVEpD/ydPddERlAXpwPOz6J7z6HHgVn3Nnm0zk5ORx77LG89NJLzJ07l8WLF3PRRRc51xglJ/P000+TmZlJaWkpxx9/PHPmzGlzOoCFCxeSmprKypUrWblyJVOnTm1+7he/+AXZ2dlEIhFmzZrFypUrufHGG/nNb37D66+/Tm5u7n77Wr58OQ8++CDvv/8+1lqOO+44TjnlFAYPHsy6detYtGgR999/PxdeeCFPPfUUl1566UH1PP744/zjH//gs88+43e/+11z6+8ll1zC/PnzOffcc6mrqyMajfLiiy/yzDPP8P7775Oamkp5+UGNbQd59913WblyJdnZ2YTD4Va/VmvWrOEXv/gF//rXv8jNzaW8vJyMjAxOPfVUnn/+ec455xwWL17MeeedR0JCzwcs8ZPYNaI3AC/jTAnzgLV2tTHmZ8Aya+0S4M/AI8aY9TgtpBe7V7GIiEjb1FIqItKOll14W3bdtdby4x//mMLCQk477TS2bt3Kzp0729zPm2++2RwOCwsLKSwsbH7uiSeeYOrUqUyZMoXVq1ezZs2atnYDwNtvv825555LWloa6enpfOMb3+Ctt94CYMyYMRx99NEATJs2jeLi4oNev3TpUvLy8sjPz2fWrFl8+OGH7N69m8rKSrZu3cq5554LQHJyMqmpqbzyyitceeWVzde2Zmdnd/h1++pXv9q8XVtfq9dee43zzz+/OXQ3bX/NNdfw4IMPAvDggw9y5ZVXdvh+A5G19gVr7eHW2nHW2l/E1t0eC6RYa+ustRdYaw+11h7bNFKviIhIf6OWUhHxhnZaNHvTOeecw80338yHH35IbW1tcwvno48+SklJCcuXLychIYGCggLq6ura3VdrrahffPEFd999N0uXLmXw4MF861vf6nA/7fXATEraNzpkMBhstfvuokWL+PTTT5u721ZUVPDUU09x4YUXtvl+rdUeCoWIRp2puQ6sOS0trfl+W1+rtvZ70kknUVxczBtvvEEkEmnuAi0iIiL+pJZSEZF2pKenc+qpp3LVVVftN8DR3r17GTJkCAkJCbz++uts2rSp3f2cfPLJPProowCsWrWKlStXAk4gTEtLY9CgQezcuZMXX3yx+TUZGRlUVla2uq9nnnmGmpoaqqurefrpp5kxY0anjicajfLXv/6VlStXUlxcTHFxMc8++yyLFi0iMzOTkSNH8swzzwBQX19PTU0Ns2fP5oEHHmgedKmp+25BQQHLly8HaHdAp7a+VrNmzeKJJ56grKxsv/0CXH755cybN0+tpCIiIgOAQqmISAfmzZvHihUruPjifZfkXXLJJSxbtoyioiIeffRRjjzyyHb3cf3111NVVUVhYSF33XUXxx57LOBMyzJlyhQmTpzIVVddxUkn7Zvm6tprr+WMM85oHuioydSpU/nWt77Fsccey3HHHcc111zDlClTOnUsb775JiNGjGDEiBHN604++WTWrFnD9u3beeSRR7jnnnsoLCzkxBNPZMeOHZx++unMmTOHoqIijj76aO6++24AbrnlFhYuXMiJJ57YPABTa9r6Wk2cOJHbbruNU045hcmTJ3PzzTfv95rdu3d3eqRjERER8S7j1kB8RUVFtmnURhGR1qxdu5bx48e7XYa44Mknn+TZZ5/lkUceafX51r43jDHLrbVtT/wqHdK5WURE4qmz52ZdUyoiIv3K9773PV588UVeeOEFt0sRERGRPqBQKiIi/cpvf/tbt0sQERGRPqRrSkVERERERMQ1CqUi0q+5dd279F/6nhAREfEXhVIR6beSk5MpKytTCJFm1lrKyspITk52uxQRERGJE11TKiL91siRI9myZQslJSVulyL9SHJyMiNHjnS7DBEREYkThVIR6bcSEhIYM2aM22WIiIiISC9S910RERERERFxjUKpiIiIiIiIuEahVERERERERFxj3BrV0hhTAmyK0+5ygdI47au/0DF5g47JG3RM3tDTY8q31ubFq5iBSOfmDumYvEHH5A06Jm/ok3Oza6E0nowxy6y1RW7XEU86Jm/QMXmDjskb/HhMA5kfP08dkzfomLxBx+QNfXVM6r4rIiIiIiIirlEoFREREREREdf4JZTe53YBvUDH5A06Jm/QMXmDH49pIPPj56lj8gYdkzfomLyhT47JF9eUioiIiIiIiDf5paVUREREREREPMjTodQYc7ox5jNjzHpjzHy36+kOY8woY8zrxpi1xpjVxpj/E1v/U2PMVmPMx7HbmW7X2hXGmGJjzCex2pfF1mUbY/5hjFkXWw52u87OMsYc0eKz+NgYU2GMuclrn5Mx5gFjzC5jzKoW61r9XIzjntjP10pjzFT3Km9bG8f0K2PMp7G6nzbGZMXWFxhjalt8Xn9wr/K2tXFMbX6vGWNujX1OnxljvuZO1e1r45geb3E8xcaYj2PrPfE5Set0bu6/dG7un3Ru9sbvfJ2be/lzstZ68gYEgQ3AWCARWAFMcLuubhzHMGBq7H4G8DkwAfgpcIvb9fXguIqB3APW3QXMj92fD/zS7Tq7eWxBYAeQ77XPCTgZmAqs6uhzAc4EXgQMcDzwvtv1d+GYZgOh2P1ftjimgpbb9ddbG8fU6vda7PfFCiAJGBP7vRh0+xg6c0wHPP9r4HYvfU66tfo56tzcj286N/fPm87N3vidr3Nz735OXm4pPRZYb63daK1tABYDc12uqcustduttR/G7lcCa4ER7lbVa+YCf4nd/wtwjou19MQsYIO1Nl4TzPcZa+2bQPkBq9v6XOYCD1vHe0CWMWZY31Taea0dk7X279bacOzhe8DIPi+sB9r4nNoyF1hsra231n4BrMf5/divtHdMxhgDXAgs6tOipDfo3Ow9Oje7TOdmb9C5uXd5OZSOADa3eLwFj58wjDEFwBTg/diqG2JdHB7wUneaGAv83Riz3BhzbWzdIdba7eCc8IEhrlXXMxez/w+olz8naPtz8cvP2FU4/1VuMsYY85Ex5g1jzAy3iuqm1r7X/PA5zQB2WmvXtVjn5c9pIPPD9+N+dG72DJ2bvUXn5v6vT8/NXg6lppV1nh1K2BiTDjwF3GStrQAWAuOAo4HtOM3nXnKStXYqcAbwXWPMyW4XFA/GmERgDvDX2Cqvf07t8fzPmDHmNiAMPBpbtR0Yba2dAtwMPGaMyXSrvi5q63vN858TMI/9/5j08uc00Pnh+7GZzs3eoHOzt37GdG72jD49N3s5lG4BRrV4PBLY5lItPWKMScA56T1qrf0bgLV2p7U2Yq2NAvfTD5v822Ot3RZb7gKexql/Z1MXk9hyl3sVdtsZwIfW2p3g/c8ppq3PxdM/Y8aYK4CvA5fY2MUQsW40ZbH7y3Gu8TjcvSo7r53vNa9/TiHgG8DjTeu8/DmJt78fW9K52VN0bvYInZu9wY1zs5dD6VLgMGPMmNh/yC4GlrhcU5fF+mv/GVhrrf1Ni/Utrw84F1h14Gv7K2NMmjEmo+k+zoXtq3A+nytim10BPOtOhT2y33+NvPw5tdDW57IEuNw4jgf2NnUl6u+MMacDPwLmWGtrWqzPM8YEY/fHAocBG92psmva+V5bAlxsjEkyxozBOaYP+rq+HjgN+NRau6VphZcyHU1sAAABMklEQVQ/J9G5ub/SudlzdG72AJ2b4/g59dYISn1xwxmB7HOcpH6b2/V08xim4zTnrwQ+jt3OBB4BPomtXwIMc7vWLhzTWJwRx1YAq5s+GyAHeBVYF1tmu11rF48rFSgDBrVY56nPCeekvR1oxPkv3tVtfS44XU/ujf18fQIUuV1/F45pPc61HE0/U3+IbXte7HtyBfAhcLbb9XfhmNr8XgNui31OnwFnuF1/Z48ptv4h4LoDtvXE56Rbm5+1zs398KZzs/u1tnMMOjd74He+zs29+zmZ2JuIiIiIiIiI9Dkvd98VERERERERj1MoFREREREREdcolIqIiIiIiIhrFEpFRERERETENQqlIiIiIiIi4hqFUhEREREREXGNQqmIiIiIiIi4RqFUREREREREXPP/Aaf2fGJGFWOcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1, 4, 5, 6, 5, 8, 9, 10]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.argmax(i) for i in model.predict(X_test)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.argmax(i) for i in y_test][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New user question encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text():\n",
    "    input_text  = ['what are you']\n",
    "    df_input = pd.DataFrame(input_text,columns=['questions'])\n",
    "    df_input\n",
    "    return df_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load artifacts \n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model-v1.h5')\n",
    "tokenizer_t = joblib.load('tokenizer_t.pkl')\n",
    "vocab = joblib.load('vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(entry):\n",
    "    tokens = entry.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens]\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_for_input(tokenizer,df,feature):\n",
    "    doc_without_stopwords = []\n",
    "    entry = df[feature][0]\n",
    "    tokens = tokenizer(entry)\n",
    "    doc_without_stopwords.append(' '.join(tokens))\n",
    "    df[feature] = doc_without_stopwords\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_input = remove_stop_words_for_input(tokenizer,df_input,'questions')\n",
    "# df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_text(tokenizer_t,df,feature):\n",
    "    t = tokenizer_t\n",
    "    entry = entry = [df[feature][0]]\n",
    "    encoded = t.texts_to_sequences(entry)\n",
    "    padded = pad_sequences(encoded, maxlen=16, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input = encode_input_text(tokenizer_t,df_input,'questions')\n",
    "# encoded_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model,encoded_input):\n",
    "    pred = np.argmax(model.predict(encoded_input))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_precausion(df_input,pred):\n",
    "    words = df_input.questions[0].split()\n",
    "    if len([w for w in words if w in vocab])==0 :\n",
    "        pred = 1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(df2,pred):\n",
    "    upper_bound = df2.groupby('labels').get_group(pred).shape[0]\n",
    "    r = np.random.randint(0,upper_bound)\n",
    "    responses = list(df2.groupby('labels').get_group(pred).response)\n",
    "    return responses[r]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(response,):\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm  BoTennis\n"
     ]
    }
   ],
   "source": [
    "df_input = get_text()\n",
    "\n",
    "#load artifacts \n",
    "tokenizer_t = joblib.load('tokenizer_t.pkl')\n",
    "vocab = joblib.load('vocab.pkl')\n",
    "\n",
    "df_input = remove_stop_words_for_input(tokenizer,df_input,'questions')\n",
    "encoded_input = encode_input_text(tokenizer_t,df_input,'questions')\n",
    "\n",
    "pred = get_pred(model,encoded_input)\n",
    "pred = bot_precausion(df_input,pred)\n",
    "\n",
    "response = get_response(df2,pred)\n",
    "bot_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_kernel",
   "language": "python",
   "name": "nlp_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
